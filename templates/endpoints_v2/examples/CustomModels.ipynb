{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a new model\n",
    "\n",
    "RayLLM supports fine-tuned versions of models in the `models` directory as well as model architectures supported by [vLLM](https://docs.vllm.ai/en/latest/models/supported_models.html). You can either bring a model from HuggingFace or artifact storage like S3, GCS.\n"
    "\n",
    "We generally recommend running the starter script via ``python main.py`` to import your own model. You can go through the below steps if your model isn't based on one of the provided model architectures and prefer to construct the model config yaml yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuring a new model\n",
    "\n",
    "To add an entirely new model to the zoo, you will need to create a new YAML file.\n",
    "This file should follow the naming convention\n",
    "`<organisation-name>--<model-name>-<model-parameters>-<extra-info>.yaml`. We recommend using one of the existing models as a template (ideally, one that is the same architecture and number of parameters as the model you are adding). The examples in the `models` directory should help you get started. You can look at the [Advanced Model Configs](./AdvancedModelConfigs.ipynb) for more details on these configurations.\n",
    "\n",
    "```yaml\n",
    "# true by default - you can set it to false to ignore this model\n",
    "# during loading\n",
    "enabled: true\n",
    "deployment_config:\n",
    "  # This corresponds to Ray Serve settings, as generated with\n",
    "  # `serve build`.\n",
    "  autoscaling_config:\n",
    "    min_replicas: 1\n",
    "    initial_replicas: 1\n",
    "    max_replicas: 8\n",
    "    target_num_ongoing_requests_per_replica: 1.0\n",
    "    metrics_interval_s: 10.0\n",
    "    look_back_period_s: 30.0\n",
    "    smoothing_factor: 1.0\n",
    "    downscale_delay_s: 300.0\n",
    "    upscale_delay_s: 90.0\n",
    "  ray_actor_options:\n",
    "    # Resources assigned to each model deployment. The deployment will be\n",
    "    # initialized first, and then start prediction workers which actually hold the model.\n",
    "    resources:\n",
    "      \"accelerator_type:A100-40G\": 0.01\n",
    "engine_config:\n",
    "  # Model id - this is a RayLLM id\n",
    "  model_id: mosaicml/mpt-7b-instruct\n",
    "  # Id of the model on Hugging Face Hub. Defaults to model_id if not specified.\n",
    "  hf_model_id: mosaicml/mpt-7b-instruct\n",
    "  # vLLM keyword arguments passed when constructing the model.\n",
    "  engine_kwargs:\n",
    "    trust_remote_code: true\n",
    "  # Optional Ray Runtime Environment configuration. See Ray documentation for more details.\n",
    "  # Add dependent libraries, environment variables, etc.\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      YOUR_ENV_VAR: \"your_value\"\n",
    "  # Optional configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads or load models not on Hugging Face Hub.\n",
    "  s3_mirror_config:\n",
    "    bucket_uri: s3://large-dl-models-mirror/models--mosaicml--mpt-7b-instruct/main-safetensors/\n",
    "  generation:\n",
    "    # Prompt format to wrap queries in. {instruction} refers to user-supplied input.\n",
    "    prompt_format:\n",
    "      system: \"{instruction}\\n\"  # System message. Will default to default_system_message\n",
    "      assistant: \"### Response:\\n{instruction}\\n\"  # Past assistant message. Used in chat completions API.\n",
    "      trailing_assistant: \"### Response:\\n\"  # New assistant message. After this point, model will generate tokens.\n",
    "      user: \"### Instruction:\\n{instruction}\\n\"  # User message.\n",
    "      default_system_message: \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"  # Default system message.\n",
    "      system_in_user: false  # Whether the system prompt is inside the user prompt. If true, the user field should include '{system}'\n",
    "      add_system_tags_even_if_message_is_empty: false  # Whether to include the system tags even if the user message is empty.\n",
    "      strip_whitespace: false  # Whether to automaticall strip whitespace from left and right of user supplied messages for chat completions\n",
    "    # Stopping sequences. The generation will stop when it encounters any of the sequences, or the tokenizer EOS token.\n",
    "    # Those can be strings, integers (token ids) or lists of integers.\n",
    "    # Stopping sequences supplied by the user in a request will be appended to this.\n",
    "    stopping_sequences: [\"### Response:\", \"### End\"]\n",
    "\n",
    "# Resources assigned to each model replica.\n",
    "scaling_config:\n",
    "  # If using multiple GPUs set num_gpus_per_worker to be 1 and then set num_workers to be the number of GPUs you want to use.\n",
    "  num_workers: 1\n",
    "  num_gpus_per_worker: 1\n",
    "  num_cpus_per_worker: 4\n",
    "  resources_per_worker:\n",
    "    # You can use custom resources to specify the instance type / accelerator type\n",
    "    # to use for the model.\n",
    "    \"accelerator_type:A100-40G\": 0.01\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a starter yaml file and you can edit the model to the specific model file that you created.\n",
    "\n",
    "You can use the Ray Serve to deploy the model. It will take a few minutes to initialize and download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run llm-serve-aws.yaml\n",
    "\n",
    "# If using GCP cloud, llm-serve-gcp.yaml conatins an example using L4 GPUs.\n",
    "# !serve run llm-serve-gcp.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adding a private model\n",
    "\n",
    "For loading a model from S3 or GCS, set `engine_config.s3_mirror_config.bucket_uri` or `engine_config.gcs_mirror_config.bucket_uri` to point to a folder containing your model and tokenizer files (`config.json`, `tokenizer_config.json`, `.bin`/`.safetensors` files, etc.) and set `engine_config.model_id` to any ID you desire in the `organization/model` format, eg. `myorganization/llama2-finetuned`. The model will be downloaded to a folder in the `<TRANSFORMERS_CACHE>/models--<organization-name>--<model-name>/snapshots/<HASH>` directory on each node in the cluster. `<HASH>` will be determined by the contents of `hash` file in the S3 folder, or default to `0000000000000000000000000000000000000000`. See the [HuggingFace transformers documentation](https://huggingface.co/docs/transformers/main/en/installation#cache-setup).\n",
    "\n",
    "For loading a model from an accessible S3 bucket:\n",
    "\n",
    "```yaml\n",
    "engine_config:\n",
    "  model_id: YOUR_MODEL_NAME\n",
    "  s3_mirror_config:\n",
    "    bucket_uri: s3://YOUR_BUCKET_NAME/YOUR_MODEL_FOLDER\n",
    "    extra_files: []\n",
    "```\n",
    "\n",
    "For loading a model from an accessible Google Cloud Storage bucket:\n",
    "\n",
    "```yaml\n",
    "engine_config:\n",
    "  model_id: YOUR_MODEL_NAME\n",
    "  s3_mirror_config:\n",
    "    bucket_uri: gs://YOUR_BUCKET_NAME/YOUR_MODEL_FOLDER\n",
    "    extra_files: []\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Format\n",
    "A prompt format is used to convert a chat completions API input into a prompt to feed into the LLM engine. The format is a dictionary where the key refers to one of the chat actors and the value is a string template for which to convert the content of the message into a string. Each message in the API input is formated into a string and these strings are assembled together to form the final prompt.\n",
    "\n",
    "The string template should include the `{instruction}` keyword, which will be replaced with message content from the ChatCompletions API.\n",
    "\n",
    "For example, if a user sends the following message for llama2-7b-chat-hf ([prompt format](models/llama/meta-llama--Llama-2-7b-chat-hf_a10g_tp1.yaml#L30-L36)):\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the capital of France?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What about Germany?\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The generated prompt that is sent to the LLM engine will be:\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "What is the capital of France? [/INST] The capital of France is Paris. </s><s>[INST] What about Germany? [/INST]\n",
    "```\n",
    "\n",
    "#### Schema\n",
    "\n",
    "The following keys are supported:\n",
    "* `system` - The system message. This is a message inserted at the beginning of the prompt to provide instructions for the LLM.\n",
    "* `assistant` - The assistant message. These messages are from the past turns of the assistant as defined in the list of messages provided in the ChatCompletions API.\n",
    "* `trailing_assistant` - The special characters that will be added to the end of the prompt before sending it to the LLM for generation. This often includes special characters that put the LLM into assitant mode (granted that model has been trained to support such special keywords). For example [vicuna](https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGML) has `Assistant: ` as a special tag that can go here.\n",
    "* `user` - The user message. This is the messages of the user as defined in the list of messages provided in the ChatCompletions API.\n",
    "\n",
    "In addition, there some configurations to control the prompt formatting behavior:\n",
    "* `default_system_message` - The default system message. This system message is used by default if one is not provided in the ChatCompletions API.\n",
    "* `system_in_user` - Whether the system prompt should be included in the user prompt. If true, the user field should include '{system}'.\n",
    "* `add_system_tags_even_if_message_is_empty` - Whether to include the system tags even if the user message is empty.\n",
    "* `strip_whitespace` - Whether to automatically strip whitespace from left and right of the content for the messages provided in the ChatCompletions API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example prompt config (Llama-based model)\n",
    "\n",
    "```\n",
    "prompt_format:\n",
    "  system: \"<<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n\"\n",
    "  assistant: \" {instruction} </s><s>\"\n",
    "  trailing_assistant: \"\"\n",
    "  user: \"[INST] {system}{instruction} [/INST]\"\n",
    "  system_in_user: true\n",
    "  default_system_message: \"\"\n",
    "stopping_sequences: []\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example prompt config (Mistral-based-model)\n",
    "\n",
    "```\n",
    "prompt_format:\n",
    "  system: \"<<SYS>>\\n{instruction}\\n<</SYS>>\\n\\n\"\n",
    "  assistant: \" {instruction} </s><s> \"\n",
    "  trailing_assistant: \" \"\n",
    "  user: \"[INST] {system}{instruction} [/INST]\"\n",
    "  system_in_user: true\n",
    "  default_system_message: \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "stopping_sequences: []\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example prompt config (Falcon-based-model)\n",
    "\n",
    "```\n",
    "prompt_format:\n",
    "  system: \"<|prefix_begin|>{instruction}<|prefix_end|>\"\n",
    "  assistant: \"<|assistant|>{instruction}<|endoftext|>\"\n",
    "  trailing_assistant: \"<|assistant|>\"\n",
    "  user: \"<|prompter|>{instruction}<|endoftext|>\"\n",
    "  default_system_message: \"Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do its best to understand exactly what is needed. It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer. That said, the assistant is practical and really does its best, and doesn't let caution get too much in the way of being useful.\"\n",
    "stopping_sequences: [\"<|prompter|>\", \"<|assistant|>\", \"<|endoftext|>\"]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
