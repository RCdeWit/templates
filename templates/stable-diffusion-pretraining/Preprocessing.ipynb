{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable data preprocessing pipeline for Stable Diffusion\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/stable-diffusion/preprocessing_architecture_v4.jpeg\" width=\"900px\">\n",
    "\n",
    "The preceding architecture diagram illustrates the data preprocessing pipeline for Stable Diffusion. \n",
    "\n",
    "Ray Data loads the data from a remote storage system, then streams the data through two processing main stages:\n",
    "1. **Transformation**\n",
    "   1. Cropping and normalizing images.\n",
    "   2. Tokenizing the text captions using a CLIP tokenizer.\n",
    "2. **Encoding**\n",
    "   1. Compressing images into a latent space using a VAE encoder.\n",
    "   2. Generating text embeddings using a CLIP model.\n",
    "\n",
    "This notebook executes a fully self-contained module, `Preprocessing.py`, that processes a small subset of the full 2 billion dataset to demonstrate the workload. You can parameterize the same module code to process the full dataset. The \"Scale to 2 billion images\" section below for a summarizes the necessary changes for read the full dataset.\n",
    "\n",
    "Run the following cell to perform the data preprocessing. The script loads the data, transforms it, and encodes the output. After the cell executes, view the two sample visualized inputs along with their corresponding outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/Preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Scale to 2 billion images\n",
    "\n",
    "The following table summarizes the changes you need to make to scale the data processing to the full dataset:\n",
    "\n",
    "| Step | Change | Description |\n",
    "| --- | --- | --- |\n",
    "| 1 | Raw Data Path | Change to point to the full dataset |\n",
    "| 2 | Data Loading Workers | Increase from 1 to 192 CPUs |\n",
    "| 3 | Transformation Workers | Increase from 1 to 192 CPUs |\n",
    "| 4 | Batch Size | Use 120 for 256x256 images and 40 for 512x512 images |\n",
    "| 5 | Encoding Workers | Increase from 0 to 48 A10-G GPUs |\n",
    "| 6 | Output Path | Change to a permanent remote storage location |\n",
    "| 7 | Run Process | Run as an Anyscale Job |\n",
    "\n",
    "In terms of infrastructure, provision 48 instances of g5.2xlarge instances for the entire process or use Anyscale's autoscaling capabilities to scale up and down as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Want to pre-train with custom data? ðŸ“ˆ\n",
    "\n",
    "If you're looking to scale your Stable Diffusion pre-training with custom data, we're here to help ðŸ™Œ !\n",
    "\n",
    "ðŸ‘‰ **[Check out this link](https://forms.gle/9aDkqAqobBctxxMa8) so we can assist you**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-template-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
