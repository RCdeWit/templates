{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Whenever we use an LLM we would like to get the highest response quality but are often restricted to a limited cost budget. Closed models, such as GPT-4, are known to be the highest quality models, but they can get very expensive especially when running them on a very number of queries. On the other hand, OSS models can be much cheaper, but their responses may not be of the same quality, especially for complex or domain-specific queries.\n",
    "\n",
    "The goal of this tutorial is to show you how you can train a \"smart router\", i.e. a model that can dynamically decide, based on the query text, whether to call a closed model or an OSS model. Here's a schematic view of a smart router:\n",
    "![Smart Router](assets/router_schema.png)\n",
    "\n",
    "The goal is to learn a smart router such that, given a set of queries, we can get responses with the highest overall quality while minimizing the total cost.\n",
    "\n",
    "# Approach\n",
    "\n",
    "We are going to train a classifier to decide, based only on the query text, whether to route the query to an OSS model vs. a closed one. In this tutorial, we will make the following design choices: \n",
    "1. We will quantify a response quality on a scale of `[1, 5]` (5-star).\n",
    "2. For simplicity, we will assume that the closed always achieves 5-start quality. \n",
    "3. We will use GPT-4 as a representative of closed models and Mixtral 8x7B for OSS models.\n",
    "\n",
    "More concurrently, let us assume that closed models have perfect a quality (5/5 score). our goal is to reduce cost significantly (say by 50%) while maintaining a high overal quality (4.8/5 score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation overview\n",
    "\n",
    "## 1) Generate labeled data\n",
    "\n",
    "## 2) Finetune a classifier on labeled data\n",
    "A smart router is a binary classifier that predicts `p(y=1|query)` -- the probability that an OSS model will produce a high quality response for the given query. \n",
    "\n",
    "## 3) Evaluate router model offline\n",
    "\n",
    "## 4) Serve the router model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Generate labaled data\n",
    "\n",
    "1. Given a set of queries, generate Mixtral model responses\n",
    "2. Given a dataset `{(prompt,  Mixtral-response)}`, use llm-as-a-judge method to score responses in `[1, 5]`\n",
    "\n",
    "For the purpose of this tutorial, we will use a the [Nectar dataset](https://huggingface.co/datasets/berkeley-nest/Nectar) which aggregates queries from multiple public datasets along with model generations of many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answers</th>\n",
       "      <th>turns</th>\n",
       "      <th>num_responses</th>\n",
       "      <th>source</th>\n",
       "      <th>good_natured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nHuman: 0.002 = 1000 \\n1 = x?\\n\\nAssistant:</td>\n",
       "      <td>[{'answer': 'To find the value of x, we can se...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[sharegpt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nHuman: 0:00\\nwhat's going on guys it's NAM...</td>\n",
       "      <td>[{'answer': 'Hello! It seems like you're shari...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[lmsys-chat-1m]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nHuman: 01011001 01001111 01010101 00100000...</td>\n",
       "      <td>[{'answer': 'The binary code you provided tran...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[anthropic-hh]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nHuman: \"012345\", \"001122\", \"ee44aa\", \"abcd...</td>\n",
       "      <td>[{'answer': 'Sure, I can help you write a func...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[lmsys-chat-1m]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nHuman: #01 You are an assistant that helps...</td>\n",
       "      <td>[{'answer': '{\n",
       "    \"thoughts\": \"Based on the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[lmsys-chat-1m]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0    \\n\\nHuman: 0.002 = 1000 \\n1 = x?\\n\\nAssistant:    \n",
       "1  \\n\\nHuman: 0:00\\nwhat's going on guys it's NAM...   \n",
       "2  \\n\\nHuman: 01011001 01001111 01010101 00100000...   \n",
       "3  \\n\\nHuman: \"012345\", \"001122\", \"ee44aa\", \"abcd...   \n",
       "4  \\n\\nHuman: #01 You are an assistant that helps...   \n",
       "\n",
       "                                             answers  turns  num_responses  \\\n",
       "0  [{'answer': 'To find the value of x, we can se...      1              7   \n",
       "1  [{'answer': 'Hello! It seems like you're shari...      1              7   \n",
       "2  [{'answer': 'The binary code you provided tran...      1              7   \n",
       "3  [{'answer': 'Sure, I can help you write a func...      1              7   \n",
       "4  [{'answer': '{\n",
       "    \"thoughts\": \"Based on the c...      1              7   \n",
       "\n",
       "            source  good_natured  \n",
       "0       [sharegpt]          True  \n",
       "1  [lmsys-chat-1m]          True  \n",
       "2   [anthropic-hh]          True  \n",
       "3  [lmsys-chat-1m]          True  \n",
       "4  [lmsys-chat-1m]          True  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load the full huggingface dataset\n",
    "# TODO: double check if you need a token to access this dataset\n",
    "nectar_data = load_dataset(\"berkeley-nest/Nectar\")\n",
    "nectar_df = nectar_data[\"train\"].to_pandas()\n",
    "nectar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe the dataset\n",
    "### explain that it contains responses by many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "gpt-3.5-turbo               191770\n",
       "gpt-4                       191487\n",
       "gpt-4-0613                  182954\n",
       "gpt-3.5-turbo-instruct      182900\n",
       "mistral-7b-instruct-v0.1    182601\n",
       "anthropic                   148844\n",
       "llama-2-7b-chat              49926\n",
       "vicuna-13b                   20962\n",
       "llama-2-70b-chat             11802\n",
       "vicuna-33b                    8160\n",
       "llama-2-13b-chat              7680\n",
       "wizardlm-13b                  7530\n",
       "alpaca-13b                    6692\n",
       "mpt-30b-chat                  6686\n",
       "koala-13b                     6457\n",
       "falcon-40b-instruct           5717\n",
       "alpaca-7b                     5675\n",
       "wizardlm-70b                  5625\n",
       "wizardlm-7b                   5615\n",
       "ultralm-13b                   5577\n",
       "ultralm-65b                   5561\n",
       "starchat                      5303\n",
       "chatglm-6b                    3860\n",
       "llama-13b                     3824\n",
       "oasst-pythia-12b              3646\n",
       "bard                          3159\n",
       "fastchat-t5-3b                3155\n",
       "dolly-v2-12b                  2817\n",
       "claude-1                      2236\n",
       "vicuna-7b                     2103\n",
       "rwkv-4-raven-14b              1983\n",
       "mpt-7b-chat                   1976\n",
       "guanaco-33b                   1559\n",
       "stablelm-tuned-alpha-7b       1520\n",
       "claude-instant-1               913\n",
       "palm-2                         901\n",
       "gpt4all-13b-snoozy             793\n",
       "claude-2                       384\n",
       "pythia-12b                     295\n",
       "default                         30\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count the number of responses for each model\n",
    "nectar_df_expanded = nectar_df.explode(\"answers\")\n",
    "nectar_df_expanded[\"model\"] = nectar_df_expanded[\"answers\"].apply(lambda x: x[\"model\"])\n",
    "\n",
    "display(nectar_df_expanded[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain that we will use a subset of the data with gpt-4 responses\n",
    "### Also explain that we will filter the data to contain single turn, good natured and preprocess prompts and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/default/templates/templates/llm-router/data_processing.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[model_name] = filtered_df['answers'].apply(lambda row: next((item['answer'] for item in row if item['model'] == model), None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/default/templates/templates/llm-router/data_processing.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['prompt'] = filtered_df['prompt'].apply(lambda prompt: pattern_end.sub(\"\", pattern_start.sub(\"\", prompt)).strip())\n",
      "/home/ray/default/templates/templates/llm-router/data_processing.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.drop(columns=['answers', 'num_responses', 'turns', 'good_natured'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from data_processing import preprocess_nectar\n",
    "\n",
    "CLOSED_LLM = \"gpt-4\"\n",
    "CLOSED_NAME = \"gpt4\"\n",
    "nectar_gpt4_df = preprocess_nectar(nectar_df, CLOSED_LLM, CLOSED_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBSET = 10\n",
    "\n",
    "# Sample a small subset from the dataset for the purpose of this tutorial\n",
    "dataset_df = nectar_gpt4_df.sample(N_SUBSET, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mixtral responses for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from online_inference import prepare_llm_queries, generate_batch_responses\n",
    "\n",
    "# store your API key in a .env file in the home directory\n",
    "load_dotenv(\"/home/ray/.env\")\n",
    "API_BASE = os.getenv(\"ANYSCALE_API_BASE\")\n",
    "API_KEY = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "\n",
    "# generate Mixtral responses with Anyscale's endpoint\n",
    "OSS_LLM = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "OSS_NAME = \"mixtral\"\n",
    "max_concurrent_queries = 25\n",
    "temperature = 0.7\n",
    "max_tokens = 512\n",
    "\n",
    "llm_queries = prepare_llm_queries(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference on 10 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 18:28:42,278\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.49.108:6379...\n",
      "2024-05-03 18:28:42,286\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-flgz3w75nljuirfmesktfhtdhx.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-05-03 18:28:42,292\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_0a357ac8c78524cad566ac791b574556.zip' (0.08MiB) to Ray cluster...\n",
      "2024-05-03 18:28:42,293\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_0a357ac8c78524cad566ac791b574556.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# queries un-processed: 9, in-progress: 1, ready: 0\n",
      "# queries un-processed: 8, in-progress: 2, ready: 0\n",
      "# queries un-processed: 7, in-progress: 3, ready: 0\n",
      "# queries un-processed: 6, in-progress: 4, ready: 0\n",
      "# queries un-processed: 5, in-progress: 5, ready: 0\n",
      "# queries un-processed: 4, in-progress: 6, ready: 0\n",
      "# queries un-processed: 3, in-progress: 7, ready: 0\n",
      "# queries un-processed: 2, in-progress: 8, ready: 0\n",
      "# queries un-processed: 1, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 1\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 1\n",
      "# queries un-processed: 0, in-progress: 7, ready: 1\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 6, ready: 1\n",
      "# queries un-processed: 0, in-progress: 6, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 1\n",
      "# queries un-processed: 0, in-progress: 4, ready: 1\n",
      "# queries un-processed: 0, in-progress: 3, ready: 1\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 1\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 1\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 0, ready: 1\n",
      "Done in 14.12sec.\n"
     ]
    }
   ],
   "source": [
    "llm_responses = generate_batch_responses(\n",
    "    API_BASE, API_KEY, OSS_LLM, llm_queries, max_concurrent_queries, temperature, max_tokens, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge responses with the dataset\n",
    "dataset_df[OSS_NAME] = dataset_df.index.map(llm_responses)\n",
    "dataset_df.head()\n",
    "dataset_df.to_json(\"assets/test_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM-as-a-judge labels\n",
    "1. explain judge template\n",
    "2. explain how we will format and call API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "Evaluate the AI assistant's proficiency in answering the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, adherence to real-world facts, depth, creativity, and level of detail of the response. You will be given a reference answer which is considered of high quality. Your assessment will have two lines: First line has a rating on a scale of 1 to 5 with a higher rating representing higher response quality. Follow strictly this format: \"[[rating]]\", for example: \"[[3]]\". Second line contains a short explanation of your rating.\n",
      "\n",
      "[Question]\n",
      "Based on the features mentioned, which hotel do you think is more preferable among the following hotels?\n",
      "Hotel A: \n",
      "- Spa: Yes\n",
      "- Swimming Pool: Yes\n",
      "- Bar: Yes\n",
      "- Restaurant: Yes\n",
      "Hotel B: \n",
      "- Spa: No\n",
      "- Swimming Pool: Yes\n",
      "- Bar: Yes\n",
      "- Restaurant: Yes\n",
      "Hotel C: \n",
      "- Spa: Yes\n",
      "- Swimming Pool: No\n",
      "- Bar: No\n",
      "- Restaurant: Yes\n",
      "Here is the SQL database code to retrieve the data:\n",
      "SELECT * FROM hotels WHERE spa='Yes' AND swimming_pool='Yes' AND bar='Yes' AND restaurant='Yes'; \n",
      "SELECT * FROM hotels WHERE spa='No' AND swimming_pool='Yes' AND bar='Yes' AND restaurant='Yes'; \n",
      "SELECT * FROM hotels WHERE spa='Yes' AND swimming_pool='No' AND bar='No' AND restaurant='Yes';\n",
      "\n",
      "[Reference Answer]\n",
      "Based on the features mentioned, Hotel A seems to be the most preferable as it includes all the amenities - a spa, swimming pool, bar, and restaurant. However, the final decision would depend on your personal preferences and needs.\n",
      "\n",
      "[Assistant Answer]\n",
      " Based on the information provided, I would say that the choice between the hotels depends on the specific features that you are looking for. However, if all the features mentioned are equally important to you, then Hotel A would be the most preferable because it includes all the facilities - a spa, swimming pool, bar, and restaurant.\n",
      "\n",
      "Here is the SQL code to retrieve the data for each hotel:\n",
      "\n",
      "For Hotel A:\n",
      "SELECT \\* FROM hotels WHERE spa='Yes' AND swimming\\_pool='Yes' AND bar='Yes' AND restaurant='Yes';\n",
      "\n",
      "For Hotel B:\n",
      "SELECT \\* FROM hotels WHERE spa='No' AND swimming\\_pool='Yes' AND bar='Yes' AND restaurant='Yes';\n",
      "\n",
      "For Hotel C:\n",
      "SELECT \\* FROM hotels WHERE spa='Yes' AND swimming\\_pool='No' AND bar='No' AND restaurant='Yes';\n",
      "\n",
      "Note that if you want to use SQL to determine which hotel has all the features, you would need to check if there are any rows returned by the first query (which corresponds to Hotel A) and no rows returned by the other two queries.\n",
      "\n",
      "Guidelines for Rating:\n",
      " - High Rating (4-5): Reserved for responses that are very close to the quality of the reference or even better.\n",
      " - Medium Rating (3): Reserved for responses that have moderate quality compared to the reference.\n",
      " - Low Rating (1-2): Allocated to response that are much lower quality compared to the reference or completely wrong.\n",
      "\n",
      "Assessment:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from online_inference import format_judge_prompt\n",
    "\n",
    "dataset_df = pd.read_json(\"assets/test_dataset.json\")\n",
    "\n",
    "with open(\"assets/judge_template.json\") as f:\n",
    "    judge_template = json.load(f)\n",
    "\n",
    "example = dataset_df.iloc[0]\n",
    "print(\n",
    "    format_judge_prompt(\n",
    "        judge_template,\n",
    "        question=example[\"prompt\"],\n",
    "        answer=example[\"mixtral\"],\n",
    "        reference_answer=example[\"gpt4\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from online_inference import prepare_llm_judge_queries, generate_batch_responses\n",
    "\n",
    "# store your API key in a .env file in the home directory\n",
    "load_dotenv(\"/home/ray/.env\")\n",
    "API_BASE = os.getenv(\"OPENAI_API_BASE\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# generate gpt4 as a judge labels with openai api\n",
    "JUDGE_LLM = \"gpt-4\"\n",
    "max_concurrent_queries = 10\n",
    "temperature = 0\n",
    "max_tokens = 256\n",
    "\n",
    "judge_queries = prepare_llm_judge_queries(dataset_df, judge_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference on 10 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 19:01:14,386\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.49.108:6379...\n",
      "2024-05-03 19:01:14,394\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-flgz3w75nljuirfmesktfhtdhx.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-05-03 19:01:14,400\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_723e3ed169539c39dc905d07aa319dbb.zip' (0.09MiB) to Ray cluster...\n",
      "2024-05-03 19:01:14,401\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_723e3ed169539c39dc905d07aa319dbb.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# queries un-processed: 9, in-progress: 1, ready: 0\n",
      "# queries un-processed: 8, in-progress: 2, ready: 0\n",
      "# queries un-processed: 7, in-progress: 3, ready: 0\n",
      "# queries un-processed: 6, in-progress: 4, ready: 0\n",
      "# queries un-processed: 5, in-progress: 5, ready: 0\n",
      "# queries un-processed: 4, in-progress: 6, ready: 0\n",
      "# queries un-processed: 3, in-progress: 7, ready: 0\n",
      "# queries un-processed: 2, in-progress: 8, ready: 0\n",
      "# queries un-processed: 1, in-progress: 8, ready: 1\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 1\n",
      "# queries un-processed: 0, in-progress: 7, ready: 1\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 6, ready: 1\n",
      "# queries un-processed: 0, in-progress: 5, ready: 1\n",
      "# queries un-processed: 0, in-progress: 4, ready: 1\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 1\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 1\n",
      "# queries un-processed: 0, in-progress: 1, ready: 1\n",
      "# queries un-processed: 0, in-progress: 0, ready: 1\n",
      "Done in 8.87sec.\n"
     ]
    }
   ],
   "source": [
    "judge_responses = generate_batch_responses(\n",
    "    API_BASE, API_KEY, JUDGE_LLM, judge_queries, max_concurrent_queries, temperature, max_tokens, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{134702: \"[[5]]\\nThe assistant's response is highly detailed, relevant, and accurate. It covers all the key points mentioned in the reference answer and adds an additional point about pay transparency. The assistant also provides a useful reminder that trends can vary based on specific contexts, which adds depth to the response.\",\n",
       " 27244: \"[[5]]\\nThe assistant's response is highly detailed, accurate, and helpful. It provides a Python-based solution to the problem, explaining each step of the algorithm in a clear and understandable manner. The assistant also mentions potential limitations and suggests possible improvements, which aligns with the reference answer. The assistant's response is on par with the reference answer in terms of quality and depth.\",\n",
       " 131095: \"[[5]]\\nThe assistant's response is highly detailed, accurate, and provides a comprehensive explanation of the JavaScript code snippet. It goes above and beyond the reference answer in terms of depth, creativity, and level of detail. The assistant's response is very helpful and relevant to the user's question.\",\n",
       " 116934: \"[[5]]\\nThe assistant's response is highly relevant and provides a detailed and creative reaction of PersonX. It aligns well with the reference answer, providing a similar level of depth and detail. The assistant also adds an extra layer of context by mentioning the school's flexibility, which enhances the quality of the response.\",\n",
       " 95277: \"[[4]]\\nThe assistant's evaluation is accurate and detailed. It correctly identifies the strengths and weaknesses of the tutor's responses, and provides a comprehensive alternative response for the tutor's last answer. However, it could have been more critical of the tutor's incorrect information in the last response, which is why it doesn't receive a perfect score.\",\n",
       " 35043: \"[[5]]\\nThe assistant's response is highly detailed, accurate, and helpful. It not only explains the change in the `load_model` function but also provides a comprehensive guide on how to use the new `FastText` object, including examples for getting word vectors, nearest neighbors, and predicting labels. This response is as good as, if not better than, the reference answer.\",\n",
       " 169249: \"[[3]]\\nThe assistant's response is moderately good. It provides a detailed comparison between driving a car and riding a bike, focusing on the economic, environmental, physical, and psychological impacts. However, the assistant's response is incomplete, cutting off in the middle of discussing the psychological impact. This makes the response less helpful and informative than the reference answer.\",\n",
       " 138869: \"[[5]]\\nThe assistant's answer is accurate, relevant, and detailed. It matches the reference answer almost exactly, providing the same information in a similarly clear and concise manner. The assistant also correctly notes that the rankings can change with new census data.\",\n",
       " 6062: \"[[5]]\\nThe assistant's response is highly relevant, detailed, and helpful. It correctly identifies Hotel A as the most preferable based on the features mentioned. It also provides the SQL code to retrieve the data for each hotel, which adds depth to the response. The assistant's response is as good as, if not better than, the reference answer.\",\n",
       " 113830: \"[[4]]\\nThe assistant's response is detailed, accurate, and provides a step-by-step guide to making homemade tomato sauce. It also includes variations and customization options. However, it lacks some depth in the tips and tricks section compared to the reference answer. The assistant could have included tips on cooking time, freezing, and tasting as you go.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_inference import parse_judge_responses\n",
    "labels, explanations = parse_judge_responses(judge_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>source</th>\n",
       "      <th>gpt4</th>\n",
       "      <th>mixtral</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>Based on the features mentioned, which hotel d...</td>\n",
       "      <td>[evol_instruct]</td>\n",
       "      <td>Based on the features mentioned, Hotel A seems...</td>\n",
       "      <td>Based on the information provided, I would sa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113830</th>\n",
       "      <td>Provide step-by-step instructions on how to cr...</td>\n",
       "      <td>[ultrachat]</td>\n",
       "      <td>Sure, here's a simple step-by-step guide on ho...</td>\n",
       "      <td>Sure, I'd be happy to help you make a homemad...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138869</th>\n",
       "      <td>What are the 10 largest cities in the US by po...</td>\n",
       "      <td>[lmsys-chat-1m]</td>\n",
       "      <td>As of the most recent data available, the 10 l...</td>\n",
       "      <td>Sure, I'd be happy to help with that! As of t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169249</th>\n",
       "      <td>Write a comparison essay of at least 500 words...</td>\n",
       "      <td>[ultrachat]</td>\n",
       "      <td>Title: A Comparative Analysis of Driving a Car...</td>\n",
       "      <td>Title: The Great Debate: Driving a Car vs. Ri...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116934</th>\n",
       "      <td>Q: You are provided with an \"Event\", \"Intent\" ...</td>\n",
       "      <td>[flan_v2_niv2]</td>\n",
       "      <td>PersonX might feel satisfied or content using ...</td>\n",
       "      <td>Person X probably feels comfortable and satis...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt           source  \\\n",
       "6062    Based on the features mentioned, which hotel d...  [evol_instruct]   \n",
       "113830  Provide step-by-step instructions on how to cr...      [ultrachat]   \n",
       "138869  What are the 10 largest cities in the US by po...  [lmsys-chat-1m]   \n",
       "169249  Write a comparison essay of at least 500 words...      [ultrachat]   \n",
       "116934  Q: You are provided with an \"Event\", \"Intent\" ...   [flan_v2_niv2]   \n",
       "\n",
       "                                                     gpt4  \\\n",
       "6062    Based on the features mentioned, Hotel A seems...   \n",
       "113830  Sure, here's a simple step-by-step guide on ho...   \n",
       "138869  As of the most recent data available, the 10 l...   \n",
       "169249  Title: A Comparative Analysis of Driving a Car...   \n",
       "116934  PersonX might feel satisfied or content using ...   \n",
       "\n",
       "                                                  mixtral  label  \n",
       "6062     Based on the information provided, I would sa...      5  \n",
       "113830   Sure, I'd be happy to help you make a homemad...      4  \n",
       "138869   Sure, I'd be happy to help with that! As of t...      5  \n",
       "169249   Title: The Great Debate: Driving a Car vs. Ri...      3  \n",
       "116934   Person X probably feels comfortable and satis...      5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['label'] = dataset_df.index.map(labels)\n",
    "dataset_df.to_json(\"assets/labeled_test_dataset.json\")\n",
    "dataset_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
