{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "When building Large Language Model (LLM) applications, we strive to balance between achieving the highest response quality while adhering to a limited cost budget. Closed models like GPT-4 are renowned for their superior quality, but they can become prohibitively expensive, especially when handling a large volume of queries. On the other hand, Open Source Software (OSS) models are more cost-effective but may not deliver the same quality, particularly for complex or domain-specific queries.\n",
    "\n",
    "A \"smart router\" addresses this challenge by processing user queries and deciding whether to route them to a closed LLM or an OSS LLM, depending on the query's complexity or domain. Here’s a schematic representation of a smart router:\n",
    "![Smart Router](assets/router_schema.png)\n",
    "\n",
    "Given a set of user queries, a smart router enables generating high-quality LLM responses while minimizing the overall cost.\n",
    "\n",
    "# Approach\n",
    "\n",
    "In this tutorial, we'll demonstrate how to train a smart router on the Anyscale platform. We make the following design choices:\n",
    "\n",
    "1. **Model Choices**: We’ll use GPT-4 as an example of a closed LLM and Mixtral-8x7B as the OSS LLM, so our smart router will route between these two models.\n",
    "2. **Response Quality Rating**: We'll quantify the quality of an LLM response on a scale of 1 to 5 stars, with higher scores indicating better quality. For simplicity, we'll assume that GPT-4 always achieves a 5-star rating, so it serves as a reference for Mixtral-8x7B.\n",
    "3. **Smart Router Model**: We'll finetune a Llama3-8B model as our smart router and leverage Anyscale's powerful API. Our research (see our [arXiv paper](put link to arxiv paper)) shows that this model offers superior routing performance compared to smaller architectures.\n",
    "\n",
    "More concretely, the objective of a smart router is to direct simple queries to Mixtral-8x7B, thereby maintaining high overall response quality (e.g., an average score of 4.8/5) while significantly reducing costs (e.g., by 50%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### Steps to Train a Smart Router\n",
    "\n",
    "1. [**Prepare labeled data**](#generate-labeled-data): We describe in this section how we generate synthetic labeled data to train the smart router model.\n",
    "\n",
    "2. [**Finetune a router model**](#finetune-router-model)\n",
    "TODO\n",
    "\n",
    "1. [**Evaluate router model offline**](#evaluate-router-model-offline)\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Labeled Data <a id=\"generate-labeled-data\"></a>\n",
    "\n",
    "Our smart router essentially functions as a binary classifier, deciding whether to route a query to GPT-4 or Mixtral-8x7B based on the query text. Initially, we considered labeled data in the format `(query, routing_label)`, where `routing_label` is 1 if the query should be routed to Mixtral-8x7B and 0 if it should be routed to GPT-4.\n",
    "\n",
    "However, our early experiments revealed that *binary labels do not provide sufficient signal for training a robust router model*. Therefore, we adopted a different labeling approach using a *1-5 scoring system*, which reflects how well Mixtral-8x7B can effectively respond to the user's query. More specifically:\n",
    "\n",
    "- **4-5**: Mixtral-8x7B produces a very strong answer, showing deep understanding, creativity, detailed insight, and high relevance.\n",
    "- **3**: Mixtral-8x7B provides an adequate answer with moderate detail, relevance, and factual accuracy.\n",
    "- **1-2**: Mixtral-8x7B struggles to produce a strong answer due to the question's difficulty, vagueness, or the model's limitations.\n",
    "\n",
    "We use labeled samples in the format `(query, score_label)`. The `routing_label` can be derived from the `score_label` by setting a score threshold for quality, i.e. `routing_label = 1 if score_label >= 4 else 0`.\n",
    "\n",
    "In the following, we will explain how we prepare our labeled dataset in detail.\n",
    "\n",
    "\n",
    "<!-- - Nectar dataset helps reduce costs by providing a variety of queries with corresponding model responses. We select queries with GPT-4 responses, generate additional responses from Mixtral-8x7B, and then obtain pairwise comparison labels using llm-as-a-judge (reference paper).\n",
    "\n",
    "- By following these steps, we collect a preference dataset of approximately 120,000 samples, with a total cost of around $700 USD. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Query Dataset\n",
    "\n",
    "We want our smart router to be effective in open-ended chat domains. So, our first step is to collect a set of generic queries from the [Nectar dataset](https://huggingface.co/datasets/berkeley-nest/Nectar). We chose the Nectar dataset for two reasons: it combines queries from many different domains, including open-ended chat, and it has responses from many models, including over 191K responses from GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answers</th>\n",
       "      <th>turns</th>\n",
       "      <th>num_responses</th>\n",
       "      <th>source</th>\n",
       "      <th>good_natured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nHuman: 0.002 = 1000 \\n1 = x?\\n\\nAssistant:</td>\n",
       "      <td>[{'answer': 'To find the value of x, we can set up a proportion using the given information:\n",
       "\n",
       "0.002/1000 = 1/x\n",
       "\n",
       "To solve for x, we can cross multiply:\n",
       "\n",
       "0.002 * x = 1000 * 1\n",
       "\n",
       "0.002x = 1000\n",
       "\n",
       "Dividing both sides by 0.002:\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "x = 500,000\n",
       "\n",
       "Therefore, 1 is equal to 500,000 in this proportion.', 'model': 'gpt-3.5-turbo', 'rank': 1.0}, {'answer': 'To solve for x in this equation, you can use cross-multiplication. Here's how:\n",
       "\n",
       "0.002 / 1 = 1000 / x\n",
       "\n",
       "Cross-multiplication gives:\n",
       "\n",
       "0.002 * x = 1000\n",
       "\n",
       "To solve for x, divide both sides by 0.002:\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "So, x equals 500,000.', 'model': 'gpt-4-0613', 'rank': 2.0}, {'answer': 'This looks like a proportion. To solve for x, you can set up a proportion equation:\n",
       "\n",
       "0.002 / 1 = 1000 / x\n",
       "\n",
       "Now, cross-multiply:\n",
       "\n",
       "0.002 \\* x = 1 \\* 1000\n",
       "\n",
       "Then, solve for x:\n",
       "\n",
       "0.002x = 1000\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "x = 500,000', 'model': 'gpt-3.5-turbo', 'rank': 3.0}, {'answer': 'If 0.002 equals 1000, then 1 would equal 500,000.', 'model': 'gpt-4', 'rank': 4.0}, {'answer': 'I apologize, but the equation you provided is not correct.\n",
       "\n",
       "0.002 is equal to 0.002, not 1000.\n",
       "\n",
       "1 is equal to x, not a number.\n",
       "\n",
       "Can you please provide a correct equation or problem for me to solve?', 'model': 'llama-2-7b-chat', 'rank': 5.0}, {'answer': '0.001 = x', 'model': 'gpt-3.5-turbo-instruct', 'rank': 6.0}, {'answer': 'It seems like you are asking for the value of x in the equation x = 1/0.002. \n",
       "\n",
       "To solve this equation, you can divide both sides by 0.002 to get: \n",
       "\n",
       "x = 1/0.002 \n",
       "x = 500 \n",
       "\n",
       "Therefore, x = 500.', 'model': 'mistral-7b-instruct-v0.1', 'rank': 7.0}]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[sharegpt]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            prompt  \\\n",
       "0  \\n\\nHuman: 0.002 = 1000 \\n1 = x?\\n\\nAssistant:    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         answers  \\\n",
       "0  [{'answer': 'To find the value of x, we can set up a proportion using the given information:\n",
       "\n",
       "0.002/1000 = 1/x\n",
       "\n",
       "To solve for x, we can cross multiply:\n",
       "\n",
       "0.002 * x = 1000 * 1\n",
       "\n",
       "0.002x = 1000\n",
       "\n",
       "Dividing both sides by 0.002:\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "x = 500,000\n",
       "\n",
       "Therefore, 1 is equal to 500,000 in this proportion.', 'model': 'gpt-3.5-turbo', 'rank': 1.0}, {'answer': 'To solve for x in this equation, you can use cross-multiplication. Here's how:\n",
       "\n",
       "0.002 / 1 = 1000 / x\n",
       "\n",
       "Cross-multiplication gives:\n",
       "\n",
       "0.002 * x = 1000\n",
       "\n",
       "To solve for x, divide both sides by 0.002:\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "So, x equals 500,000.', 'model': 'gpt-4-0613', 'rank': 2.0}, {'answer': 'This looks like a proportion. To solve for x, you can set up a proportion equation:\n",
       "\n",
       "0.002 / 1 = 1000 / x\n",
       "\n",
       "Now, cross-multiply:\n",
       "\n",
       "0.002 \\* x = 1 \\* 1000\n",
       "\n",
       "Then, solve for x:\n",
       "\n",
       "0.002x = 1000\n",
       "\n",
       "x = 1000 / 0.002\n",
       "\n",
       "x = 500,000', 'model': 'gpt-3.5-turbo', 'rank': 3.0}, {'answer': 'If 0.002 equals 1000, then 1 would equal 500,000.', 'model': 'gpt-4', 'rank': 4.0}, {'answer': 'I apologize, but the equation you provided is not correct.\n",
       "\n",
       "0.002 is equal to 0.002, not 1000.\n",
       "\n",
       "1 is equal to x, not a number.\n",
       "\n",
       "Can you please provide a correct equation or problem for me to solve?', 'model': 'llama-2-7b-chat', 'rank': 5.0}, {'answer': '0.001 = x', 'model': 'gpt-3.5-turbo-instruct', 'rank': 6.0}, {'answer': 'It seems like you are asking for the value of x in the equation x = 1/0.002. \n",
       "\n",
       "To solve this equation, you can divide both sides by 0.002 to get: \n",
       "\n",
       "x = 1/0.002 \n",
       "x = 500 \n",
       "\n",
       "Therefore, x = 500.', 'model': 'mistral-7b-instruct-v0.1', 'rank': 7.0}]   \n",
       "\n",
       "   turns  num_responses      source  good_natured  \n",
       "0      1              7  [sharegpt]          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries with GPT-4 responses: 191487\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_and_display_nectar\n",
    "nectar_df = load_and_display_nectar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Preprocessing\n",
    "\n",
    "We will use a subset of the Nectar data that includes responses from GPT-4, as these will be used to generate scores (as seen below). We will process this data by focusing on single-turn conversations, filtering for good-natured interactions, and cleaning up the prompts and responses to maintain high quality. Additionally, we will sample a small subset from the dataset for the purpose of this tutorial; however, you can skip sampling to work with the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import preprocess_nectar\n",
    "\n",
    "nectar_gpt4_df = preprocess_nectar(nectar_df, \"gpt-4\", \"gpt4\")\n",
    "\n",
    "# Sample a small subset from the dataset for the purpose of this tutorial\n",
    "N_SUBSET = 10\n",
    "dataset_df = nectar_gpt4_df.sample(N_SUBSET, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Labeling\n",
    "\n",
    "We don't have human labels for scores, so we will use the [LLM-as-a-Judge approach](https://arxiv.org/abs/2306.05685). GPT-4 will act as an evaluator, reviewing the query and Mixtral's response to provide a score from 1-5. As shown in the paper, the most robust way to get labels is by providing a reference answer for comparison. Here, GPT-4's own response serves as the reference, and Mixtral's response is evaluated against it.\n",
    "\n",
    "There are two main steps in this process:\n",
    "1. **Generate Mixtral-8x7B responses for all queries**: We will use an online batch-inference method utilizing Ray and Anyscale endpoints.\n",
    "2. **Generate LLM-as-a-Judge labels**: We will ask GPT-4 to evaluate the Mixtral responses against its own reference answers and provide a score from 1-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mixtral-8x7B Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference on 10 queries...\n",
      "# queries un-processed: 9, in-progress: 1, ready: 0\n",
      "# queries un-processed: 8, in-progress: 2, ready: 0\n",
      "# queries un-processed: 7, in-progress: 3, ready: 0\n",
      "# queries un-processed: 6, in-progress: 4, ready: 0\n",
      "# queries un-processed: 5, in-progress: 5, ready: 0\n",
      "# queries un-processed: 4, in-progress: 6, ready: 0\n",
      "# queries un-processed: 3, in-progress: 7, ready: 0\n",
      "# queries un-processed: 2, in-progress: 8, ready: 0\n",
      "# queries un-processed: 1, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 10, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 1\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 9, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 1\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 8, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 1\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 7, ready: 0\n",
      "# queries un-processed: 0, in-progress: 6, ready: 1\n",
      "# queries un-processed: 0, in-progress: 6, ready: 0\n",
      "# queries un-processed: 0, in-progress: 6, ready: 0\n",
      "# queries un-processed: 0, in-progress: 6, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 1\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 5, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 1\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 4, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 1\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 3, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 1\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 2, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 1\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 1, ready: 0\n",
      "# queries un-processed: 0, in-progress: 0, ready: 1\n",
      "Done in 41.43sec.\n",
      "Dataset overview with Mixtral respinses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>source</th>\n",
       "      <th>gpt4</th>\n",
       "      <th>mixtral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>Based on the features mentioned, which hotel d...</td>\n",
       "      <td>[evol_instruct]</td>\n",
       "      <td>Based on the features mentioned, Hotel A seems...</td>\n",
       "      <td>Based on the information provided, I would ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113830</th>\n",
       "      <td>Provide step-by-step instructions on how to cr...</td>\n",
       "      <td>[ultrachat]</td>\n",
       "      <td>Sure, here's a simple step-by-step guide on ho...</td>\n",
       "      <td>Sure, I'd be happy to help you make a homemad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138869</th>\n",
       "      <td>What are the 10 largest cities in the US by po...</td>\n",
       "      <td>[lmsys-chat-1m]</td>\n",
       "      <td>As of the most recent data available, the 10 l...</td>\n",
       "      <td>Here are the 10 largest cities in the U.S. by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169249</th>\n",
       "      <td>Write a comparison essay of at least 500 words...</td>\n",
       "      <td>[ultrachat]</td>\n",
       "      <td>Title: A Comparative Analysis of Driving a Car...</td>\n",
       "      <td>Title: The Great Debate: Driving a Car vs. Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116934</th>\n",
       "      <td>Q: You are provided with an \"Event\", \"Intent\" ...</td>\n",
       "      <td>[flan_v2_niv2]</td>\n",
       "      <td>PersonX might feel satisfied or content using ...</td>\n",
       "      <td>Person X probably feels comfortable and focus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt           source  \\\n",
       "6062    Based on the features mentioned, which hotel d...  [evol_instruct]   \n",
       "113830  Provide step-by-step instructions on how to cr...      [ultrachat]   \n",
       "138869  What are the 10 largest cities in the US by po...  [lmsys-chat-1m]   \n",
       "169249  Write a comparison essay of at least 500 words...      [ultrachat]   \n",
       "116934  Q: You are provided with an \"Event\", \"Intent\" ...   [flan_v2_niv2]   \n",
       "\n",
       "                                                     gpt4  \\\n",
       "6062    Based on the features mentioned, Hotel A seems...   \n",
       "113830  Sure, here's a simple step-by-step guide on ho...   \n",
       "138869  As of the most recent data available, the 10 l...   \n",
       "169249  Title: A Comparative Analysis of Driving a Car...   \n",
       "116934  PersonX might feel satisfied or content using ...   \n",
       "\n",
       "                                                  mixtral  \n",
       "6062     Based on the information provided, I would ne...  \n",
       "113830   Sure, I'd be happy to help you make a homemad...  \n",
       "138869   Here are the 10 largest cities in the U.S. by...  \n",
       "169249   Title: The Great Debate: Driving a Car vs. Ri...  \n",
       "116934   Person X probably feels comfortable and focus...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to assets/test_dataset.jsonl.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from online_inference import generate_mixtral_responses\n",
    "\n",
    "# store your API key in a .env file in the home directory\n",
    "load_dotenv(\"/home/ray/.env\")\n",
    "anyscale_api_key = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "\n",
    "dataset_df = generate_mixtral_responses(dataset_df, anyscale_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LLM-as-a-judge labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at an example query we will send to GPT-4 for judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "Evaluate the AI assistant's proficiency in answering the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, adherence to real-world facts, depth, creativity, and level of detail of the response. You will be given a reference answer which is considered of high quality. Your assessment will have two lines: First line has a rating on a scale of 1 to 5 with a higher rating representing higher response quality. Follow strictly this format: \"[[rating]]\", for example: \"[[3]]\". Second line contains a short explanation of your rating.\n",
      "\n",
      "[Question]\n",
      "Q: You are provided with an \"Event\", \"Intent\" related to PersonX. Guess a reaction/reaction of PersonX about the given event and their intention.\n",
      "Event:PersonX uses ___ in class. Intent: 1) to use his prefered writing implement\n",
      "A:\n",
      "\n",
      "[Reference Answer]\n",
      "PersonX might feel satisfied or content using their preferred writing implement in class, as it aligns with their intention to utilize a comfortable and desired tool for writing. \n",
      "Confidence: 85%\n",
      "\n",
      "[Assistant Answer]\n",
      " Person X probably feels comfortable and focused in class, as they are using their preferred writing implement. This may help them engage more effectively in the class activities and feel more satisfied with their learning experience.\n",
      "\n",
      "Guidelines for Rating:\n",
      " - High Rating (4-5): Reserved for responses that are very close to the quality of the reference or even better.\n",
      " - Medium Rating (3): Reserved for responses that have moderate quality compared to the reference.\n",
      " - Low Rating (1-2): Allocated to response that are much lower quality compared to the reference or completely wrong.\n",
      "\n",
      "Assessment:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from data_utils import format_judge_prompt\n",
    "\n",
    "\n",
    "with open(\"assets/judge_template.json\") as f:\n",
    "    judge_template = json.load(f)\n",
    "\n",
    "example = dataset_df.iloc[4]\n",
    "print(\n",
    "    format_judge_prompt(\n",
    "        judge_template,\n",
    "        question=example[\"prompt\"],\n",
    "        answer=example[\"mixtral\"],\n",
    "        reference_answer=example[\"gpt4\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply a similar online batch-inference method to generate our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from online_inference import generate_llm_judge_labels\n",
    "\n",
    "# store your API key in a .env file in the home directory\n",
    "load_dotenv(\"/home/ray/.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "dataset_df = generate_llm_judge_labels(dataset_df, openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Finetune a router model <a id=\"finetune-router-model\"></a>\n",
    "\n",
    "In this section we will explain how you can finetune an LLM as a smart router. While we have described above how to  Note that while our data contains `gpt4_response` and `mixtral_response`, we will only use the pair `(query, label)` in training our model. At the end of the day, the smart router is supposed to rely on the query text only to infer which model to route to. Our approach is simple: we train a 5-way classifier to predict the score given the query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_df = pd.read_json(\n",
    "    \"/mnt/user_storage/templates_data/labeled_full_dataset.jsonl\",\n",
    "    lines=True,\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import visualize_label_distribution\n",
    "\n",
    "# visualize the label distribution\n",
    "visualize_label_distribution(dataset_df, key=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why we higher % for 4-5 scores (Mixtral is competitive with GPT-4 06-2023 version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that if the score >= 4 then we will route to the OSS model (the response quality is good enough), otherwise, we will route the closed model. Under this assumption, the data distribution looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"routing_label\"] = dataset_df[\"label\"].apply(lambda x: 1 if x >= 4 else 0)\n",
    "visualize_label_distribution(dataset_df, key=\"routing_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import split_dataset, balance_dataset\n",
    "\n",
    "# split data to train/validation sets\n",
    "train_df, validation_df = split_dataset(dataset_df, validation_size=5000)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(validation_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to train and validate classification tasks on balanced datasests, so that the model and metrics are unbiased to one label. Let's create a balanced train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_df = balance_dataset(train_df, key=\"routing_label\")\n",
    "balanced_validation_df = balance_dataset(validation_df, key=\"routing_label\")\n",
    "\n",
    "visualize_label_distribution(balanced_train_df, key=\"routing_label\")\n",
    "\n",
    "print(f\"Train size: {len(balanced_train_df)}\")\n",
    "print(f\"Validation size: {len(balanced_validation_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune an LLM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format data in Anyscale finetuning format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import prepare_ft_messages\n",
    "\n",
    "balanced_train_df[\"messages\"] = prepare_ft_messages(balanced_train_df)\n",
    "balanced_validation_df[\"messages\"] = prepare_ft_messages(balanced_validation_df)\n",
    "\n",
    "# for debugging, here's what the messages look like:\n",
    "display(balanced_train_df[\"messages\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Train llm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Batch inference with router model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate router performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "1. Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "router_predictions = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(123)\n",
    "router_predictions[\"Random\"] = rng.uniform(0, 1, len(validation_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_metrics import plot_quality_cost_curve\n",
    "\n",
    "oss_model_scores = validation_df['label'].to_numpy()\n",
    "closed_model_scores = np.ones(len(validation_df['label'])) * 5.0\n",
    "\n",
    "plot_quality_cost_curve(oss_model_scores, closed_model_scores, router_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
