{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying hyperparameters\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: 10 minutes\n",
    "\n",
    "This guide will focus on how you can customize your fine-tuning run by modifying the various hyperparameters configurable. Make sure you've read the [basic fine-tuning guide](../../README.md) for better context. \n",
    "\n",
    "We provide a number of options to configure via the training YAML.\n",
    "\n",
    "# Example YAML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## GPU resources\n",
    "\n",
    "Configuring GPU resources to be used is one of the most important pre-requisities for training. There are two fields in our YAML that are relevant:\n",
    "\n",
    "```yaml\n",
    "num_devices: 16 # number of GPUs \n",
    "worker_resources:\n",
    "  accelerator_type:A10G: 0.001 # specifies GPU type available, and a minimum allocation per worker\n",
    "```\n",
    "\n",
    "Internally, our fine-tuning code will launch Ray workers with each being allocated one GPU. The cluster will be auto-scaled if needed to meet the requirements. The different GPU types you can specify can depend on the specific Anyscale Cloud. The value you specify for the accelerator type does not matter much, as long as it's non-zero (so that each worker is allocated a GPU) and less than or equal to 1 (so that the requested number of GPUs is the same as `num_devices`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Learning rate\n",
    "\n",
    "There are two entities of interest here: the actual learning rate value itself and the particular learning rate scheduler you use. The parameters you can control in the YAML are below: \n",
    "\n",
    "```yaml\n",
    "learning_rate: 1e-4\n",
    "lr_scheduler_type: cosine\n",
    "num_warmup_steps: 10\n",
    "```\n",
    "\n",
    "In the above config, the training run would use a cosine learning rate schedule (the default) with an initial warmup of 10 steps (the default). The peak learning rate would be 1e-4 (the value specified). \n",
    "\n",
    "We support both `'linear'` and `'cosine'` schedules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "The batch size for training and validation depends on the below parameters:\n",
    "\n",
    "```yaml\n",
    "num_devices: 8\n",
    "train_batch_size_per_device: 16\n",
    "eval_batch_size_per_device: 16\n",
    "```\n",
    "The effective batch size for training would be `train_batch_size_per_device * num_devices`. For the hardware you specify, the amount you can push `train_batch_size_per_device` / `eval_batch_size_per_device` depends on dataset statistics (average sequence length) and the context length used. For a context length of 512 and the default NVIDIA A10 GPUs, the per-device batch size of 16 is a good default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA configs\n",
    "We support all the LoRA parameters you can configure in [ü§óPEFT](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig):\n",
    "\n",
    "```yaml\n",
    "lora_config:\n",
    "  r: 8\n",
    "  lora_alpha: 16\n",
    "  lora_dropout: 0.05\n",
    "  target_modules:\n",
    "    - q_proj\n",
    "    - v_proj\n",
    "    - k_proj\n",
    "    - o_proj\n",
    "    - gate_proj\n",
    "    - up_proj\n",
    "    - down_proj\n",
    "    - embed_tokens\n",
    "    - lm_head\n",
    "  task_type: \"CAUSAL_LM\"\n",
    "  modules_to_save: []\n",
    "  bias: \"none\"\n",
    "  fan_in_fan_out: false\n",
    "  init_lora_weights: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
