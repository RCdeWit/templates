{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Open-weight LLMs with Anyscale\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: 20 minutes\n",
    "\n",
    "Fine-tuning LLMs is an easy and cost-effective way to tailor their capabilities towards niche applications with high-acccuracy. While Ray and RayTrain offer generic primitives for building such workloads, at Anyscale we have created a higher-level library called _LLMForge_ that builds on top of Ray and other open-source libraries to provide an easy to work with interface for fine-tuning and training LLMs. \n",
    "\n",
    "This template is a guide on how to use LLMForge for fine-tuning LLMs. For serving finetuned models you can see the [LLM serving template](../endpoints_v2/README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "- [What is LLMForge?](#what-is-llmforge)\n",
    "  - [Configurations](#configurations)\n",
    "    - [Default Mode](#default-mode)\n",
    "    - [Custom Mode](#custom-mode)\n",
    "  - [Models Supported in default Mode](#models-supported-in-default-mode)\n",
    "- [Summary of Features in Custom Mode](#summary-of-features-in-custom-mode)\n",
    "- [Examples](#examples)\n",
    "  - [Default](#default)\n",
    "  - [Custom](#custom)\n",
    "- [Cookbooks](#cookbooks)\n",
    "- [End-to-end Examples](#end-to-end-examples)\n",
    "- [LLMForge Versions](#llmforge-versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LLMForge?\n",
    "\n",
    "LLMForge is a library that implements a collection of design patterns that use Ray, RayTrain, and RayData in combination with other open-source libraries (e.g. Deepspeed, ü§ó Huggingface accelerate, transformers, etc.) to provide an easy to use library for fine-tuning LLMs. In addition to these design patterns, it offers tight integrations with the Anyscale platform, such as model registry, streamlined deployment, observability, Anyscale's job submission, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "LLMForge workloads are specified using YAML configurations ([documentation here](https://docs.anyscale.com/reference/finetuning-config-api)). The library offers two main modes: `default` and `custom`.\n",
    "\n",
    "#### Default Mode\n",
    "Similar to OpenAI's finetuning experience, the `default` mode provides a minimal and efficient setup. It allows you to quickly start a finetuning job by setting just a few parameters (`model_id` and `train_path`). All other settings are optional and will be automatically selected based on dataset statistics and predefined configurations.\n",
    "\n",
    "#### Custom Mode\n",
    "The `custom` mode offers more flexibility and control over the finetuning process, allowing for advanced optimizations and customizations. You need to provide more configurations to setup this mode (e.g. prompt format, hardware, batch size, etc.)\n",
    "\n",
    "Here's a comparison of the two modes:\n",
    "\n",
    "| Feature | Default Mode | Custom Mode |\n",
    "|---------|-----------|-------------|\n",
    "| Ideal For | Prototyping what's possible, focusing on dataset cleaning, finetuning, and evaluation pipeline | Optimizing model quality by controlling more parameters, hardware control |\n",
    "| Command | `llmforge anyscale finetune config.yaml --default` | `llmforge anyscale finetune config.yaml` |\n",
    "| Model Support | Popular models with their prompt format (e.g., `meta-llama/Meta-Llama-3-8B-Instruct`)* | Any HuggingFace model, any prompt format (e.g., `meta-llama/Meta-Llama-Guard-2-8B`) |\n",
    "| Task Support | Instruction tuning for multi-turn chat | Causal language modeling, Instruction tuning, Classification|\n",
    "| Data Format | Supports chat-style datasets, with fixed prompt formats per model | Supports chat-style datasets, with flexible prompt format |\n",
    "| Hardware | Automatically selected (limited by availability) | User-configurable |\n",
    "| Fine-tuning type| Only supports LoRA (Rank-8, all linear layers) | User-defined LoRA and Full-parameter |\n",
    "\n",
    "*NOTE: old models will get deprecated\n",
    "\n",
    "Choose the mode that best fits your project requirements and level of customization needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Supported in Default Mode\n",
    "\n",
    "Default mode supports a list of \"core\" models, with a fixed cluster type of 8xA100-80G. For each model we only support context lengths of 512 up to Max. context length in increments of 2x (i.e. 512, 1024, ...). Here are the supported models and their configurations:\n",
    "\n",
    "|Model family | model_id(s) | Max. context lengths |\n",
    "|------------|----------|----------------------|\n",
    "|Llama-3.1| `meta-llama/Meta-Llama-3.1-8B-Instruct` | 4096 |\n",
    "|Llama-3.1| `meta-llama/Meta-Llama-3.1-70B-Instruct`  | 4096 |\n",
    "|Llama-3| `meta-llama/Meta-Llama-3-8B-Instruct` | 4096 |\n",
    "|Llama-3| `meta-llama/Meta-Llama-3-70B-Instruct`| 4096 |\n",
    "|Mistral| `mistralai/Mistral-Nemo-Instruct-2407`  | 4096 |\n",
    "|Mistral| `mistralai/Mistral-7B-Instruct-v0.3` | 4096 |\n",
    "|Mixtral| `mistralai/Mixtral-8x7B-Instruct-v0.1` | 4096 |\n",
    "\n",
    "\n",
    "Note: \n",
    "- Cluster type for all models: 8xA100-80G\n",
    "- Supported context length for models: 512 up to max. context length of each model in powers of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Features in Custom Mode\n",
    "\n",
    "### ‚úÖ Support both Full parameter and LoRA\n",
    "\n",
    "* LoRA with different configurations, ranks, layers, etc. (Anything supported by huggingface transformers)\n",
    "* Full-parameter with multi-node training support\n",
    "    \n",
    "### ‚úÖ State of the art performance related features:\n",
    "\n",
    "* Gradient checkpointing\n",
    "* Mixed precision training\n",
    "* Flash attention v2\n",
    "* Deepspeed support (zero-DDP sharding)\n",
    "\n",
    "### ‚úÖ Unified chat data format with flexible prompt format support enabling finetuning for:\n",
    "\n",
    "\n",
    "#### Use-case: Multi-turn chat, Instruction tuning, Classification:\n",
    "\n",
    "Example data format (JSON):\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Howdy!\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the type of this model?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"[[1]]\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Prompt Format for llama-3-instruct (YAML):\n",
    "\n",
    "```yaml\n",
    "system: \"<|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "user: \"<|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "assistant: \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{instruction}<|eot_id|>\"\n",
    "system_in_user: False\n",
    "```\n",
    "\n",
    "#### Use-case: Casual language modeling (aka continued pre-training), custom prompt formats (e.g. Llama-guard):\n",
    "\n",
    "Example Continued pre-training (JSON):\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Once upon a time ...\"},\n",
    "    ],\n",
    "},\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "Prompt Format for doing nothing except concatenation:\n",
    "\n",
    "```yaml\n",
    "system: \"{instruction}\"\n",
    "user: \"{instruction}\"\n",
    "assistant: \"{instruction}\"\n",
    "system_in_user: False\n",
    "```\n",
    "\n",
    "### ‚úÖ Flexible task support: \n",
    "\n",
    "* Causal language modeling: Each token predicted based on all past tokens.\n",
    "* Instruction tuning: Only assistant tokens are predicted based on past tokens.\n",
    "* Classification: Only special tokens in the assistant message are predicted based on past tokens.\n",
    "* (Coming soon) Preference tuning: Use the contrast between chosen and rejected messages to improve the model.\n",
    "\n",
    "### ‚úÖ Support for multi-stage continuous fine-tuning\n",
    "\n",
    "* Fine-tune on one dataset, then continue fine-tuning on another dataset, for iterative improvements.\n",
    "* Do continued pre-training on one dataset, then chat-style fine-tuning on another dataset.\n",
    "* (Coming soon) Do continued pre-training on one dataset followed by iterations of supervised-finetuning and preference tuning on independent datasets.\n",
    "\n",
    "### ‚úÖ Support for context length extension\n",
    "\n",
    "* Extend the context length of the model via methods like RoPE scaling.\n",
    "\n",
    "### ‚úÖ Configurability of hyper-parameters\n",
    "\n",
    "* Full control over learning hyper-parameters such as learning rate, n_epochs, batch size, etc.\n",
    "\n",
    "### ‚úÖ Anyscale and third-party integrations\n",
    "\n",
    "* (Coming soon) Model registry: \n",
    "    * SDK for accessing finetuned models for creating automated pipelines \n",
    "    * More streamlined deployment flow when finetuned on Anyscale\n",
    "* Monitoring and observability:\n",
    "    * Take advantage of standard logging frameworks such as Weights and Biases\n",
    "    * Use of ray dashboard and anyscale loggers for debugging and monitoring the training process\n",
    "* Anyscale jobs integration: Use Anyscale's job submission API to programitically submit long-running jobs through LLMForge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Here are some examples for default mode and custom mode:\n",
    "\n",
    "### Default Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------- \n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct in default mode (LoRA rank 8). Just giving the dataset.\n",
    "\n",
    "**Command:**\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/default/meta-llama/Meta-Llama-3-8B-Instruct-simple.yaml --default\n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------- \n",
    "\n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct in default mode but also control parameters like `learning_rate` and `num_epochs`. \n",
    "\n",
    "**Command:**\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/default/meta-llama/Meta-Llama-3-8B-Instruct-custom.yaml --default\n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_epochs: 3\n",
    "learning_rate: 1e-4         \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "**Task:** \n",
    "\n",
    "Fine-tune llama-3-8b-instruct (a \"core\" model) in custom mode on 16xA10s (auto mode uses 8xA100-80G) with context length of 512.\n",
    "\n",
    "\n",
    "**Command:** \n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/custom/meta-llama--Meta-Llama-3-8B-Instruct/lora/16xA10-512.yaml \n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "context_length: 512\n",
    "deepspeed:\n",
    "  config_path: deepspeed_configs/zero_3_offload_optim+param.json\n",
    "worker_resources:\n",
    "  accelerator_type:A10G: 0.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------\n",
    "**Task:** \n",
    "\n",
    "Fine-tune gemma-2-27b in custom mode on 8xA100-80G.\n",
    "\n",
    "\n",
    "**Command:** \n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune training_configs/custom/google--gemma-2-27b-it/lora/8xA100-80G-512.yaml \n",
    "```\n",
    "\n",
    "**Config:**\n",
    "\n",
    "```yaml\n",
    "model_id: google/gemma-2-27b-it\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_devices: 8\n",
    "worker_resources:\n",
    "  accelerator_type:A100-80G: 0.001\n",
    "generation_config:\n",
    "  prompt_format:\n",
    "    system: \"{instruction} + \"\n",
    "    assistant: \"<start_of_turn>model\\n{instruction}<end_of_turn>\\n\"\n",
    "    trailing_assistant: \"<start_of_turn>model\\n\"\n",
    "    user: \"<start_of_turn>user\\n{system}{instruction}<end_of_turn>\\n\"\n",
    "    system_in_user: True\n",
    "    bos: \"<bos>\"\n",
    "    default_system_message: \"\"\n",
    "  stopping_sequences: [\"<end_of_turn>\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples can be found in `./training_configs`. For specific features read [cookbooks](#cookbooks) and [end-to-end examples](#end-to-end-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbooks\n",
    "\n",
    "After you are with the above, you can find recipies that extend the functionality of this template under the cookbooks folder:\n",
    "\n",
    "* [Bring your own data](cookbooks/bring_your_own_data/README.md): Everything you need to know about using custom datasets for fine-tuning.\n",
    "* [Bring any huggingface model and prompt format](cookbooks/bring_any_hf_model/README.md): Learn how you can finetune any ü§óHugging Face model with a custom prompt format (chat template). \n",
    "* [LoRA vs. full-parameter training](cookbooks/continue_from_checkpoint/README.md): Learn the differences between LoRA and full-parameter training and how to configure both.\n",
    "* [Continue fine-tuning from a previous checkpoint](cookbooks/continue_from_checkpoint/README.md): A detailed guide on how you can use a previous checkpoint for another round of fine-tuning.\n",
    "* [Modifying hyperparameters](cookbooks/modifying_hyperparameters/README.md): A brief guide on customization of your fine-tuning job.\n",
    "* [Optimizing Cost and Performance for Finetuning](cookbooks/optimize_cost/README.md): A detailed guide on default performance-related parameters and how you can optimize throughput for training on your own data.\n",
    "* [Run finetuning as Anyscale Job](cookbooks/launch_as_anyscale_job/README.md): A detailed guide on how to submit a finetuning workflow as a job (outside the context of workspaces.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Examples\n",
    "\n",
    "Here is a list of end-to-end examples that involve more steps such as data preprocessing, evaluation, etc but with a main focus on improving model quality via fine-tuning.\n",
    "\n",
    "* [Fine-tuning for Function calling on custom data](end-to-end-examples/fine-tune-function-calling/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMForge Versions\n",
    "\n",
    "Here is a list of LLMForge image versions:\n",
    "\n",
    "| version | image_uri |\n",
    "|---------|-----------|\n",
    "| `0.5.2`  | `localhost:5555/anyscale/llm-forge:0.5.2` |\n",
    "| `0.5.1`  | `localhost:5555/anyscale/llm-forge:0.5.1` |\n",
    "| `0.5.0.1`  | `localhost:5555/anyscale/llm-forge:0.5.0.1-ngmM6BdcEdhWo0nvedP7janPLKS9Cdz2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
