{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3, Mistral and Mixtral with Anyscale\n",
    "\n",
    "**⏱️ Time to complete**: 2.5 hours for 7/8B models (9 hours for 13B, 25 hours for 70B)\n",
    "\n",
    "The guide below walks you through the steps required for fine-tuning of LLM models. This template provides an easy to configure solution for ML Platform teams, Infrastructure engineers, and Developers to fine-tune LLMs.\n",
    "\n",
    "### Popular base models to fine-tune\n",
    "\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7b\n",
    "\n",
    "A full list of supported models is in the FAQ section.\n",
    "\n",
    "## Step 1 - Launch a fine-tuning job\n",
    "\n",
    "We have provided different example configurations under the `training_configs` directory for different base models and accelerator types. You can use these as a starting point for your own fine-tuning jobs.\n",
    "\n",
    "[Optional] you can get a WandB API key from [WandB](https://wandb.ai/authorize) to track the fine-tuning process.\n",
    "\n",
    "Next, you can launch a fine-tuning job where the WandB API key is passed as an environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] You can set the WandB API key to track model performance\n",
    "# !export WANDB_API_KEY={YOUR_WANDB_API_KEY}\n",
    "\n",
    "# Launch a LoRA fine-tuning job for Llama 3 8B with 16 A10s\n",
    "!python main.py training_configs/lora/llama-3-8b.yaml\n",
    "\n",
    "# Launch a full-param fine-tuning job for Llama 3 8B with 16 A10s\n",
    "# !python main.py training_configs/full_param/llama-3-8b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the command runs, you can monitor a number of built-in metrics out of the box in the `Metrics` tab under `Ray Dashboard`, such as the number of GPU nodes and GPU utilization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/assets/gpu-usage.png\" width=500px/>\n",
    "\n",
    "Depending on whether you are running LoRA or full-param fine-tuning, you can continue with step 2(a) or step 2(b).\n",
    "\n",
    "## Step 2(a) - Serving the LoRA fine-tuned model\n",
    "\n",
    "Upon the job completion, you can see the LoRA weight storage location and model ID in the log, such as the one below:\n",
    "\n",
    "```shell\n",
    "Note: LoRA weights will also be stored in path {ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning under meta-llama/Llama-2-8b-chat-hf:sql:12345 bucket.\n",
    "```\n",
    "\n",
    "You can specify this URI as the dynamic_lora_loading_path ([docs](https://docs.endpoints.anyscale.com/preview/examples/deploy-llms/#more-guides) in the llm serving template, and then query the endpoint.\n",
    "\n",
    "Note: Such LoRA model IDs follow the format `{base_model_id}:{suffix}:{id}`\n",
    "\n",
    "## Step 2(b) - Serving the full-parameter fine-tuned model\n",
    "\n",
    "Once the fine-tuning job is complete, you can view the stored full-parameter fine-tuned checkpoint at the very end of the job logs. Here is an example fine-tuning job output:\n",
    "\n",
    "```shell\n",
    "Best checkpoint is stored in:\n",
    "{ANYSCALE_ARTIFACT_STORAGE}/username/llmforge-finetuning/meta-llama/Llama-2-70b-hf/TorchTrainer_2024-01-25_18-07-48/TorchTrainer_b3de9_00000_0_2024-01-25_18-07-48/checkpoint_000000\n",
    "```\n",
    "\n",
    "Follow the [Learn how to bring your own models](https://docs.endpoints.anyscale.com/preview/examples/deploy-llms/#more-guides) section under the llm serving template to serve this fine-tuned model with the specified storage uri.\n",
    "\n",
    "## Frequently asked questions\n",
    "\n",
    "### Where can I view the bucket where my LoRA weights are stored?\n",
    "\n",
    "All the LoRA weights are stored under the URI `${ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning` where `ANYSCALE_ARTIFACT_STORAGE` is an environmental variable.\n",
    "\n",
    "### How can I fine-tune using my own data?\n",
    "\n",
    "You can open the file under `training_configs` and update `train_path` and `valid_path` to your training and evaluation file.\n",
    "\n",
    "### How do I customize the fine-tuning job?\n",
    "\n",
    "You can edit the values, such as `context_length`, `num_epoch`, `train_batch_size_per_device` and `eval_batch_size_per_device` to customize the fine-tuning job.\n",
    "\n",
    "In addition, the deepspeed configs are provided in case you would\n",
    "like to customize them.\n",
    "\n",
    "### What's the full list of supported models?\n",
    "\n",
    "The following models can currently be fine-tuned with this template.\n",
    "\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7b\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- meta-llama/Meta-Llama-3-8B\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct\n",
    "\n",
    "### What if I want to use another dataset? \n",
    "\n",
    "The training configs provided in this template all train on the GSM8k which requires a context length of 512 tokens.\n",
    "How to ensure the correct format is described in https://docs.endpoints.anyscale.com/fine-tuning/dataset-prep.\n",
    "You can replace the s3 buckets in the training configs in this template with paths to your own dataset.\n",
    "\n",
    "### I have the right model, context length and everything. Can I optimize compute cost?\n",
    "\n",
    "Optimizing your fine-tuning runs for compute cost is a non-trivial problem.\n",
    "The default configs in this template require the following compute:\n",
    "Llama-3-8B and Mistral require 16 A10Gs. Llama-3-70B and Mixtral require 32 A10Gs.\n",
    "If you want different compute, we *suggest* the following workflow to find a suitable configuration:\n",
    "\n",
    "* Select some model, context length, fine-tuning technique (LoRA or full-paramter), etc. that suit the problem you want to solve\n",
    "* Start off with compute that you think will give you good flops/$. If you are not sure, here is a rough guideline:\n",
    "    * g5 nodes for high availability\n",
    "    * p4d/p4de nodes for lower availability but better flops/$\n",
    "    * Anything more modern if you have the means of acquiring them\n",
    "* Do some iterations of trial and error on batch-size and deepspeed settings to fit the workload while keeping other settings fixed\n",
    "    * Start with batch sizes of 1\n",
    "    * Use deepspeed stage 3\n",
    "    * Try to use deepspeed offloading only if it reduces the minimum number of instances you have to use\n",
    "    * Use as few instances as possible\n",
    "    * Increase batch size as much as possible until your instances don't OOM\n",
    "\n",
    "We do not guarantee that this will give you optimal settings, but have found this workflow to be helpful ourselves in the past.\n",
    "\n",
    "### How can I get even more control?\n",
    "\n",
    "This template fine-tunes with Anyscale's library `llmforge`, which uses [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [Ray Train](https://docs.ray.io/en/latest/train/train.html) for distributed training.\n",
    "You can study main.py to find out how we call the `lmforge dev finetune` API with a YAML that specifies the fine-tuning workload.\n",
    "You can call `lmforge dev finetune` yourself and gain control by modifying the training config YAMLs in this template.\n",
    "For anything that goes beyond using `llmforge`, you can build your own fine-tuning stack on Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
