{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3, Mistral and Mixtral with Anyscale\n",
    "\n",
    "**⏱️ Time to complete**: 2.5 hours for 7/8B models (9 hours for 13B, 25 hours for 70B)\n",
    "\n",
    "The guide below walks you through the steps required for fine-tuning of LLM models. This template provides an easy to configure solution for ML Platform teams, Infrastructure engineers, and Developers to fine-tune LLMs.\n",
    "\n",
    "### Popular base models to fine-tune\n",
    "\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7b\n",
    "\n",
    "A full list of supported models is in the FAQ section.\n",
    "\n",
    "## Step 1 - Launch a fine-tuning job\n",
    "\n",
    "We have provided different example configurations under the `training_configs`\n",
    "directory for different base models and accelerator types. You can use these as a\n",
    "starting point for your own fine-tuning jobs.\n",
    "\n",
    "[Optional] you can get a WandB API key from [WandB](https://wandb.ai/authorize) to track the fine-tuning process.\n",
    "\n",
    "Next, you can launch a fine-tuning job where the WandB API key is passed as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/default/templates/templates/fine-tune-llm_v2/main.py\", line 77, in <module>\n",
      "    main()\n",
      "  File \"/home/ray/default/templates/templates/fine-tune-llm_v2/main.py\", line 69, in main\n",
      "    subprocess.run(parsed_args, check=True)\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/home/ray/anaconda3/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'WANDB_API_KEY=887c5376e455712420914f6917c63174d92aadda'\n"
     ]
    }
   ],
   "source": [
    "# [Optional] You can set the WandB API key to track model performance\n",
    "# !export WANDB_API_KEY={YOUR_WANDB_API_KEY}\n",
    "\n",
    "# Launch a full-param fine-tuning job for Llama 3 8B with 16 A10s\n",
    "!python main.py training_configs/full_param/llama-3-8b-512-16xa10.yaml\n",
    "\n",
    "# Launch a LoRA fine-tuning job for Llama 3 8B with 16 A10s\n",
    "# !python main.py training_configs/lora/llama-2-8b-512-16xa10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the command runs, you can monitor a number of built-in metrics out of the box in the `Metrics` tab under `Ray Dashboard`, such as the number of GPU nodes and GPU utilization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/assets/gpu-usage.png\" width=500px/>\n",
    "\n",
    "Depending on whether you are running LoRA or full-param fine-tuning, you can continue with step 2(a) or step 2(b).\n",
    "\n",
    "## Step 2(a) - Serving the LoRA fine-tuned model\n",
    "\n",
    "Upon the job completion, you can see the LoRA weight storage location and model ID in the log, such as the one below:\n",
    "\n",
    "```shell\n",
    "Note: LoRA weights will also be stored in path {ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning under meta-llama/Llama-2-8b-chat-hf:sql:12345 bucket.\n",
    "```\n",
    "\n",
    "You can specify this URI as the dynamic_lora_loading_path ([docs](https://docs.endpoints.anyscale.com/preview/examples/deploy-llms/#more-guides) in the llm serving template, and then query the endpoint.\n",
    "\n",
    "Note: Such LoRA model IDs follow the format `{base_model_id}:{suffix}:{id}`\n",
    "\n",
    "## Step 2(b) - Serving the full-parameter fine-tuned model\n",
    "\n",
    "Once the fine-tuning job is complete, you can view the stored full-parameter fine-tuned checkpoint at the very end of the job logs. Here is an example fine-tuning job output:\n",
    "\n",
    "```shell\n",
    "Best checkpoint is stored in:\n",
    "{ANYSCALE_ARTIFACT_STORAGE}/username/llmforge-finetuning/meta-llama/Llama-2-70b-hf/TorchTrainer_2024-01-25_18-07-48/TorchTrainer_b3de9_00000_0_2024-01-25_18-07-48/checkpoint_000000\n",
    "```\n",
    "\n",
    "Follow the [Learn how to bring your own models](https://docs.endpoints.anyscale.com/preview/examples/deploy-llms/#more-guides) section under the llm serving template to serve this fine-tuned model with the specified storage uri.\n",
    "\n",
    "## Frequently asked questions\n",
    "\n",
    "### Where can I view the bucket where my LoRA weights are stored?\n",
    "\n",
    "All the LoRA weights are stored under the URI `${ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning` where `ANYSCALE_ARTIFACT_STORAGE` is an environmental variable.\n",
    "\n",
    "### How can I fine-tune using my own data?\n",
    "\n",
    "You can open the file under `training_configs` and update `train_path` and `valid_path` to your training and evaluation file.\n",
    "\n",
    "### How do I customize the fine-tuning job?\n",
    "\n",
    "You can edit the values, such as `context_length`, `num_epoch`, `train_batch_size_per_device` and `eval_batch_size_per_device` to customize the fine-tuning job.\n",
    "\n",
    "In addition, the deepspeed configs are provided in case you would\n",
    "like to customize them.\n",
    "\n",
    "### How can I gain more control over fine-tuning?\n",
    "\n",
    "This template fine-tunes with Anyscale's library `llmforge`.\n",
    "You can study main.py to find out how we call the `lmforge dev finetune` API with a YAML that specifies the fine-tuning workload.\n",
    "You can gain more control by modifying the YAMLs in this template.\n",
    "\n",
    "### What's the full list of supported models?\n",
    "\n",
    "The following models can be fine-tuned with `llmforge`.\n",
    "\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7b\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- meta-llama/Meta-Llama-3-8B\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
