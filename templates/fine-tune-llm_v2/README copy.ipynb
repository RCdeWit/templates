{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Open-weight LLMs with Anyscale\n",
    "\n",
    "**‚è±Ô∏è Time to complete**: N/A\n",
    "\n",
    "Fine-tuning LLMs is an easy and cost-effective way to tailor their capabilities towards niche applications with high-acccuracy. While Ray and RayTrain offer a generic primitives for building such workloads, at Anyscale we have created a higher-level library called _LLMForge_ that builds on top of Ray and other open-source libraries to provide an easy to work with interface for fine-tuning and training LLMs. \n",
    "\n",
    "This template is a guide on how to use LLMForge for fine-tuning LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LLMForge?\n",
    "\n",
    "LLMForge is a library that implements a collection of design patterns that use Ray, RayTrain, and RayData in combination with other open-source libraries (e.g. Deepspeed, ü§ó Huggingface accelerate, transformers, etc.) to provide an easy to use library for fine-tuning LLMs. In addition to these design patterns, it offers tight integrations with the Anyscale platform, such as model registery, streamlined deployment, observability, Anyscale's job submission, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "LLMForge workloads are specified using YAML configurations ([documentation here](https://docs.anyscale.com/reference/finetuning-config-api)). The library offers two main modes: `auto` and `custom`.\n",
    "\n",
    "#### Auto Mode\n",
    "Similar to OpenAI's finetuning experience, the `auto` mode provides a minimal and efficient setup. It allows you to quickly start a finetuning job by setting just a few parameters (`model_id` and `train_path`). All other settings are optional and will be automatically selected based on dataset statistics and predefined configurations.\n",
    "\n",
    "#### Custom Mode\n",
    "The `custom` mode offers more flexibility and control over the finetuning process, allowing for advanced optimizations and customizations. You need to provide more configurations to setup this mode (e.g. prompt format, hardware, batch size, etc.)\n",
    "\n",
    "Here's a comparison of the two modes:\n",
    "\n",
    "| Feature | Auto Mode | Custom Mode |\n",
    "|---------|-----------|-------------|\n",
    "| Ideal For | Prototyping what's possible, focusing on dataset cleaning, finetuning, and evaluation pipeline | Optimizing model quality by controlling more parameters, hardware control |\n",
    "| Command | `llmforge anyscale finetune config.yaml --auto` | `llmforge anyscale finetune config.yaml` |\n",
    "| Model Support | Popular chat-format models (e.g., `meta-llama/Meta-Llama-3-8B-Instruct`) | Any HuggingFace model, any format (e.g., `meta-llama/Meta-Llama-Guard-2-8B`) |\n",
    "| Task Support | Instruction tuning for multi-turn chat | CausalLM, instruction tuning, classification, preference tuning |\n",
    "| Data Format | Fixed for chat-style data | Flexible, supports various prompt formats |\n",
    "| Hardware | Automatically selected (limited by availability) | User-configurable |\n",
    "| Fine-tuning type| Only supports LoRA (Rank-8, all linear layers) | User-defined LoRA and Full-parameter |\n",
    "\n",
    "Choose the mode that best fits your project requirements and level of customization needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Supported in Auto Mode\n",
    "\n",
    "Auto mode supports a select list of models, with a fixed cluster type of 8xA100-80G. Here are the supported models and their configurations:\n",
    "\n",
    "| model_id | Supported context lengths |\n",
    "|-------|-----------------|\n",
    "| meta-llama/Meta-Llama-3-8B-Instruct |  512, 1024, 2048, 4096 |\n",
    "| meta-llama/Meta-Llama-3-70B-Instruct  | 512, 1024, 2048, 4096 |\n",
    "| mistralai/Mistral-7B-Instruct-v0.2 | 512, 1024, 2048, 4096 |\n",
    "\n",
    "Note: \n",
    "- Cluster type for all models: 8xA100-80G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Auto\n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune example_configs/auto/llama-3-8b/simple.yaml --auto\n",
    "```\n",
    "\n",
    "Config is simple:\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "logger:\n",
    "    wandb:\n",
    "        project: ...\n",
    "        entity: ...        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto with more flexibility\n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune example_configs/auto/llama-3-8b/custom.yaml --auto\n",
    "```\n",
    "\n",
    "I can control some of the training knobs such as learning rate, num_epochs, etc.\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_epochs: 3\n",
    "learning_rate: 1e-4\n",
    "logger:\n",
    "    wandb:\n",
    "        project: ...\n",
    "        entity: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom with full-parameter \n",
    "\n",
    "```bash\n",
    "llmforge anyscale finetune example_configs/custom/llama-3-8b/full-g5-48xlarge.yaml \n",
    "```\n",
    "\n",
    "```yaml\n",
    "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "train_path: s3://...\n",
    "valid_path: s3://...\n",
    "num_epochs: 3\n",
    "learning_rate: 1e-4\n",
    "logger:\n",
    "    wandb:\n",
    "        project: ...\n",
    "        entity: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The guide below walks you through the steps required for fine-tuning of LLMs. This template provides an easy to configure solution for ML Platform teams, Infrastructure engineers, and Developers to fine-tune LLMs.\n",
    "\n",
    "### Popular base models to fine-tune*\n",
    "\n",
    "- meta-llama/Meta-Llama-3-8B (Full-param and LoRA)\n",
    "- meta-llama/Meta-Llama-3-70B (Full-param and LoRA)\n",
    "- mistralai/Mistral-7B (Full-param and LoRA)\n",
    "- mistralai/Mixtral-8x7B (LoRA only)\n",
    "\n",
    "*Any model that has the same architecture and parameter count as above can be finetuned. A subset of popular variants of these models are provided out of the box on this template. For this subset, the Huggingface model id is enough. But for models beyond this list, the location to the weights must be provided. \n",
    "\n",
    "A full list of out-of-the-box supported models is in the [FAQ](#faqs) section. In the end we provide more guides in form of [cookbooks](#cookbooks) and [end-to-end examples](#end-to-end-examples) that provide more detailed information about using this template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Launch a fine-tuning run in [workspaces](https://docs.anyscale.com/platform/workspaces/)\n",
    "\n",
    "We provide example configurations under the `./training_configs` directory for different base models and accelerator types. You can use these as a starting point for your own fine-tuning jobs. The full-list of public configurations that are customizable see [Anyscale docs](https://docs.anyscale.com/reference/finetuning-config-api).\n",
    "\n",
    "**Optional**: You can get a WandB API key from [WandB](https://wandb.ai/authorize) to track the fine-tuning process. If not provided, you can only track the experiments through the standard output logs.\n",
    "\n",
    "Next, you can launch a fine-tuning job with your WandB API key passed as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17 20:30:24,933] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# [Optional] You can set the WandB API key to track model performance\n",
    "# import os\n",
    "# os.environ[\"WANDB_API_KEY\"]=\"YOUR_WANDB_API_KEY\"\n",
    "\n",
    "# Launch a LoRA fine-tuning job for Llama 3 8B with 16 A10s\n",
    "!llmforge anyscale finetune training_configs/lora/llama-3-8b.yaml\n",
    "\n",
    "# Launch a full-param fine-tuning job for Llama 3 8B with 16 A10s\n",
    "# !llmforge anyscale finetune  training_configs/full_param/llama-3-8b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LLMForge` is an Anyscale CLI and library that is installed on this workspace so that you can quickly experiment and customize various LLM finetuning experiments by simply modifying a config file. For extensive documentation around what is supported through the config refer to [docs](https://docs.anyscale.com/reference/finetuning-config-api/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-28 14:38:55,193] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Usage: llmforge anyscale finetune [OPTIONS] CONFIG\n",
      "\n",
      "  Runs finetuning with LLMForge on a given configuration file.\n",
      "\n",
      "  This is supposed to be used in the context of Anyscale platform either in\n",
      "  Workspace or as entrypoint of a job.\n",
      "\n",
      "  Args:\n",
      "\n",
      "      CONFIG: Path to the YAML configuration. See docs for more info.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "# To get help on the CLI\n",
    "!llmforge anyscale finetune --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the command runs, you can monitor a number of built-in metrics in the `Metrics` tab under `Ray Dashboard`, such as the number of GPU nodes and GPU utilization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/templates/main/templates/fine-tune-llm_v2/assets/gpu-usage.png\" width=500px/>\n",
    "\n",
    "Depending on whether you are running LoRA or full-param fine-tuning, you can continue with step 2(a) or step 2(b). To learn more about LoRA vs. full-parameter, see the cookbooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2(a) - Serving the LoRA fine-tuned model\n",
    "\n",
    "Upon the job completion, you can see the LoRA weight storage location and model ID in the log, such as the one below:\n",
    "\n",
    "```shell\n",
    "Note: LoRA weights will also be stored in path {ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning under meta-llama/Llama-2-8b-chat-hf:sql:12345 bucket.\n",
    "```\n",
    "\n",
    "You can specify this URI as the dynamic_lora_loading_path [docs](https://docs.anyscale.com/examples/deploy-llms#more-guides) in the llm serving template, and then query the endpoint.\n",
    "\n",
    "> Note: Such LoRA model IDs follow the format `{base_model_id}:{suffix}:{id}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2(b) - Serving the full-parameter fine-tuned model\n",
    "\n",
    "Once the fine-tuning job is complete, you can view the stored full-parameter fine-tuned checkpoint at the very end of the job logs. Here is an example fine-tuning job output:\n",
    "\n",
    "```shell\n",
    "Best checkpoint is stored in:\n",
    "{ANYSCALE_ARTIFACT_STORAGE}/username/llmforge-finetuning/meta-llama/Llama-2-70b-hf/TorchTrainer_2024-01-25_18-07-48/TorchTrainer_b3de9_00000_0_2024-01-25_18-07-48/checkpoint_000000\n",
    "```\n",
    "\n",
    "Follow the [Learn how to bring your own models](https://docs.anyscale.com/examples/deploy-llms#more-guides) section under the llm serving template to serve this fine-tuned model with the specified storage uri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbooks\n",
    "\n",
    "After you are with the above, you can find recipies that extend the functionality of this template under the cookbooks folder:\n",
    "\n",
    "* [Bring your own data](cookbooks/bring_your_own_data/README.md): Everything you need to know about using custom datasets for fine-tuning.\n",
    "* [Bring any huggingface model and prompt format](cookbooks/bring_any_hf_model/README.md): Learn how you can finetune any ü§óHugging Face model with a custom prompt format (chat template). \n",
    "* [Continue fine-tuning from a previous checkpoint](cookbooks/continue_from_checkpoint/README.md): A detailed guide on how you can use a previous checkpoint for another round of fine-tuning.\n",
    "* [LoRA vs. full-parameter training](cookbooks/continue_from_checkpoint/README.md): Learn the differences between LoRA and full-parameter training and how to configure both.\n",
    "* [Modifying hyperparameters](cookbooks/modifying_hyperparameters/README.md): A brief guide on customization of your fine-tuning job.\n",
    "* [Optimizing Cost and Performance for Finetuning](cookbooks/optimize_cost/README.md): A detailed guide on default performance-related parameters and how you can optimize throughput for training on your own data.\n",
    "* [Run finetuning as Anyscale Job](cookbooks/launch_as_anyscale_job/README.md): A detailed guide on how to submit a finetuning workflow as a job (outside the context of workspaces.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Examples\n",
    "\n",
    "Here is a list of end-to-end examples that involve more steps such as data preprocessing, evaluation, etc but with a main focus on improving model quality via fine-tuning.\n",
    "\n",
    "* [Fine-tuning for Function calling on custom data](end-to-end-examples/fine-tune-function-calling/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMForge Versions\n",
    "\n",
    "Here is a list of LLMForge image versions:\n",
    "\n",
    "| version | image_uri |\n",
    "|---------|-----------|\n",
    "| `0.5.2`  | `localhost:5555/anyscale/llm-forge:0.5.2` |\n",
    "| `0.5.1`  | `localhost:5555/anyscale/llm-forge:0.5.1` |\n",
    "| `0.5.0.1`  | `localhost:5555/anyscale/llm-forge:0.5.0.1-ngmM6BdcEdhWo0nvedP7janPLKS9Cdz2` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FAQs\n",
    "\n",
    "### Where can I view the bucket where my LoRA weights are stored?\n",
    "\n",
    "All the LoRA weights are stored under the URI `${ANYSCALE_ARTIFACT_STORAGE}/lora_fine_tuning` where `ANYSCALE_ARTIFACT_STORAGE` is an environmental variable in your workspace.\n",
    "\n",
    "### What's the full list of supported models?\n",
    "\n",
    "This is a growing list but it includes the following models:\n",
    "\n",
    "- meta-llama/Meta-Llama-3-8B\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- meta-llama/Meta-Llama-3-70B\n",
    "- meta-llama/Meta-Llama-3-70B-Instruct\n",
    "- meta-llama/Llama-2-7b-hf\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- codellama/CodeLlama-34b-Instruct-hf\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "\n",
    "In general, any model that is compatible with the architecture of these models can be fine-tuned using the same configs as the base models.\n",
    "\n",
    "NOTE: currently mixture of expert models (such as `mistralai/Mixtral-8x7B)` only support LoRA fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
