{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Function calling on custom data.\n",
    "\n",
    "**⏱️ Time to complete**: 5 hours for 7/8B models (11 hours for 13B, 27 hours for 70B)\n",
    "\n",
    "Function calling is an important capability of large language models. Connecting your model to external tools is at the heart of many LLM applications. In Anyscale Endpoints, you can use the [function calling API](https://docs.anyscale.com/preview/endpoints/text-generation/function-calling) to enable any model to use external tools. This is made possible [through JSON mode](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features). However, it is beneficial to have *native* function calling capabilities in your model through fine-tuning on a relevant function calling dataset. JSON-mode-based function calling can only guarantee that the output is in the right schema, and can also be more expensive than a regular chat completion. However, fine-tuning on a function calling dataset can improve the model's capabilities with intent recognition (understanding when to call and when not to call a tool) and function call accuracy (employing the right function with accurate parameters) in addition to structured data formatting (formatting the function call json in the correct schema).  Fine-tuning would also be the only systematic way to improve performance on use-case-specific data. \n",
    "\n",
    "In this example, we demonstrate fine-tuning on [Glaive's function calling dataset](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2?row=0) using Anyscale Endpoints. The dataset consists of about 113,000 examples of synthetically generated function calling data.  The dataset composition is given below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/distr_glaive_pie.png\" alt=\"Distribution\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Data Preprocessing](#step-1-data-preprocessing)\n",
    "2. [Finetuning](#step-2-fine-tuning)\n",
    "3. [Serving](#step-3-serving)\n",
    "4. [Evaluation](#step-4-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import datasets\n",
    "import ray.data \n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fc_utils.preprocessing import initial_mapper, pprint_example, preprocess, save_to_jsonl, TOOL_CALL_TAGS, TOOL_RESULT_TAGS\n",
    "from fc_utils.response_parsers import OpenAIResponseParser, AnyscaleResponseParser\n",
    "from fc_utils.eval_utils import evaluate_model\n",
    "from fc_utils.test_utils import get_evaluation_dataset\n",
    "from fc_utils.plot_utils import plot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "We'll use Ray Data for scalable data processing. First let's load the dataset from the HuggingFace Hub and inspect a few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:03:48,712\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "hf_ds = datasets.load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train\").shuffle(seed=21) \n",
    "hf_ds_subset =  hf_ds.select(range(int(len(hf_ds)*0.10))) # sample only 10% of the dataset\n",
    "ray_ds = ray.data.from_huggingface(hf_ds_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:03:52,191\tINFO dataset.py:2370 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2024-05-17 01:03:52,194\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-05-17_01-03-46_632348_53956/logs/ray-data\n",
      "2024-05-17 01:03:52,194\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd18f9127484b8fb5de9dd91f96eac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab185c679c554c7f941974b755fc5421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94msystem: \u001b[0mSYSTEM: You are a helpful assistant with access to the following functions. Use them if required -\n",
      "{\n",
      "    \"name\": \"create_reminder\",\n",
      "    \"description\": \"Create a reminder for a specific date and time\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"reminder_text\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"The content of the reminder\"\n",
      "            },\n",
      "            \"reminder_date\": {\n",
      "                \"type\": \"string\",\n",
      "                \"format\": \"date\",\n",
      "                \"description\": \"The date of the reminder\"\n",
      "            },\n",
      "            \"reminder_time\": {\n",
      "                \"type\": \"string\",\n",
      "                \"format\": \"time\",\n",
      "                \"description\": \"The time of the reminder\"\n",
      "            }\n",
      "        },\n",
      "        \"required\": [\n",
      "            \"reminder_text\",\n",
      "            \"reminder_date\",\n",
      "            \"reminder_time\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\u001b[92mchat: \u001b[0mUSER: I need to set a reminder for my doctor's appointment.\n",
      "\n",
      "\n",
      "ASSISTANT: Sure, I can help with that. Could you please provide me with the date and time of your appointment? <|endoftext|>\n",
      "\n",
      "\n",
      "USER: The appointment is on 2022-09-15 at 10:00 AM.\n",
      "\n",
      "\n",
      "ASSISTANT: <functioncall> {\"name\": \"create_reminder\", \"arguments\": '{\"reminder_text\": \"Doctor's appointment\", \"reminder_date\": \"2022-09-15\", \"reminder_time\": \"10:00\"}'} <|endoftext|>\n",
      "\n",
      "\n",
      "FUNCTION RESPONSE: {\"status\": \"success\", \"message\": \"Reminder for 'Doctor's appointment' on 2022-09-15 at 10:00 AM has been created successfully.\"}\n",
      "\n",
      "\n",
      "ASSISTANT: Your reminder for the doctor's appointment on 2022-09-15 at 10:00 AM has been created successfully. You will be notified at the specified time. <|endoftext|>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint_example(ray_ds.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice, each sample has two entries: system and chat. This dataset is already formatted in specific way (e.g. using USER, \\<|endoftext|\\> and other tokens). To enable fine-tuning on various open source models we need to convert each row to a more general format like the OpenAI chat format, which is the preferred format for fine-tuning instruction-tuned models on Anyscale ([dataset format guide](https://docs.endpoints.anyscale.com/endpoints/fine-tuning/dataset-prep)). Let's first bring this dataset into the conversation format and inspect how that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:08:30,192\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-05-17_01-03-46_632348_53956/logs/ray-data\n",
      "2024-05-17 01:08:30,193\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(initial_mapper)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4b322ed4044b038ba93336c0e7b0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(initial_mapper) 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdf1df058b64d7c9ff7d192925e116e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e1610b028497caf215fadee9a6aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91msystem: \u001b[0mYou are a helpful assistant.\n",
      "\u001b[92muser: \u001b[0mHi, I need help with calculating the tip for my bill. The total amount is $50 and I want to give a 20% tip.\n",
      "\u001b[94massistant: \u001b[0m[TOOL_CALLS] [ {\"name\": \"calculate_tip\", \"arguments\": '{\"total\": 50, \"tip_percentage\": 20}'} ] [/TOOL_CALLS]\n",
      "\u001b[93mtool: \u001b[0m[{\"tip_amount\": 10}]\n",
      "\u001b[94massistant: \u001b[0mBased on the total bill amount of $50 and a tip percentage of 20%, the tip amount you should give is $10. \n",
      "\u001b[95mTool list: \u001b[0m[{\"name\": \"calculate_tip\", \"description\": \"Calculate the tip amount based on bill total and tip percentage\", \"parameters\": {\"type\": \"object\", \"properties\": {\"total\": {\"type\": \"number\", \"description\": \"The total bill amount\"}, \"tip_percentage\": {\"type\": \"number\", \"description\": \"The percentage of tip to be given\"}}, \"required\": [\"total\", \"tip_percentage\"]}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial preprocessing to get to the conversation format\n",
    "openai_fmt_ds = ray_ds.map(initial_mapper)\n",
    "pprint_example(openai_fmt_ds.take(1)[0], keys=[\"messages\", \"tools\"]) # inspect one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we made use of special indicators \"\\[TOOL_CALLS\\]\" and \"\\[/TOOL_CALLS\\]\" to denote a tool call (as supposed a regular JSON output from the model). We'll now further process this conversation format and make it compatible with Anyscale Endpoints. The role \"tool\" will be converted to the role \"user\" with a special indicator to highlight that this is a tool response. Further, the tool list will be included in the system prompt with special indicators. The following code block handles the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:08:32,700\tINFO streaming_executor.py:112 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-05-17_01-03-46_632348_53956/logs/ray-data\n",
      "2024-05-17 01:08:32,701\tINFO streaming_executor.py:113 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(final_mapper)->Filter(filter_func)->MapBatches(drop_columns)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2e85cd231a4c7e8a451afa5f6f5c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(final_mapper)->Filter(filter_func)->MapBatches(drop_columns) 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c55cc945b64672b3e953cd43cf6f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d357764426f4ee8ab9770fc475c5c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91msystem: \u001b[0mYou are a helpful assistant.[TOOL_LIST] [{\"name\": \"get_stock_price\", \"description\": \"Get the current stock price\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\", \"description\": \"The stock symbol, e.g. AAPL\"}}, \"required\": [\"symbol\"]}},{\"name\": \"calculate_mortgage_payment\", \"description\": \"Calculate the monthly mortgage payment based on loan details\", \"parameters\": {\"type\": \"object\", \"properties\": {\"loan_amount\": {\"type\": \"number\", \"description\": \"The amount of the loan\"}, \"interest_rate\": {\"type\": \"number\", \"description\": \"The annual interest rate\"}, \"loan_term\": {\"type\": \"integer\", \"description\": \"The loan term in years\"}}, \"required\": [\"loan_amount\", \"interest_rate\", \"loan_term\"]}}] [/TOOL_LIST]\n",
      "\u001b[92muser: \u001b[0mCan you tell me the current stock price of AAPL?\n",
      "\u001b[94massistant: \u001b[0m[TOOL_CALLS] [ {\"name\": \"get_stock_price\", \"arguments\": '{\"symbol\": \"AAPL\"}'} ] [/TOOL_CALLS]\n",
      "\u001b[92muser: \u001b[0m[TOOL_RESULTS] [{\"status\": \"success\", \"data\": {\"symbol\": \"AAPL\", \"price\": 150.75}}] [/TOOL_RESULTS]\n",
      "\u001b[94massistant: \u001b[0mThe current stock price of AAPL is $150.75. \n",
      "\u001b[92muser: \u001b[0mWhat about the stock price of MSFT?\n",
      "\u001b[94massistant: \u001b[0m[TOOL_CALLS] [ {\"name\": \"get_stock_price\", \"arguments\": '{\"symbol\": \"MSFT\"}'} ] [/TOOL_CALLS]\n",
      "\u001b[92muser: \u001b[0m[TOOL_RESULTS] [{\"status\": \"success\", \"data\": {\"symbol\": \"MSFT\", \"price\": 295.40}}] [/TOOL_RESULTS]\n",
      "\u001b[94massistant: \u001b[0mThe current stock price of MSFT is $295.40. \n",
      "\u001b[92muser: \u001b[0mThank you for the information.\n",
      "\u001b[94massistant: \u001b[0mYou're welcome! If you have any other questions, feel free to ask. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complete preprocessing step\n",
    "processed_ds = preprocess(ray_ds) \n",
    "pprint_example(processed_ds.take(1)[0]) # inspect one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a train, validation and test split and save the datasets in the `jsonl` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = processed_ds.split_proportionately([0.8, 0.1])\n",
    "test_ds, _  = test_ds.split_at_indices([200]) # restrict to 200 examples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9036, 1129, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect final counts\n",
    "train_ds.count(), val_ds.count(), test_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets to jsonl format\n",
    "save_to_jsonl(train_ds, \"glaiveai-function-calling-v2-train.jsonl\")\n",
    "save_to_jsonl(val_ds, \"glaiveai-function-calling-v2-val.jsonl\")\n",
    "save_to_jsonl(test_ds, \"glaiveai-function-calling-v2-test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning, you have two options with Anyscale:\n",
    "1. Fine-tuning on the Anyscale Platform through our fine-tuning template \n",
    "    - This would be the preferred route for those wishing to get more flexibility in choice of models and hyperparameters, better monitoring, etc.\n",
    "2. Fine-tuning through Anyscale's serverless endpoints\n",
    "    - A quick and easy way to fine-tune a model via an OpenAI compatiable SDK.\n",
    "\n",
    "For this guide, we will use `Llama-3-8B-Instruct` as the base model for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2(a): Fine-tuning on the Anyscale Platform\n",
    "\n",
    "Head over to the Anyscale Platform: https://console.anyscale.com/v2 and spin up the \"Fine-tune LLMs\" template (under \"AI application templates\")\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/templates.png\" alt=\"Templates\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Follow the instructions to run your fine-tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2(b): Fine-tuning through serverless endpoints\n",
    "First, obtain your credentials from the [Anyscale platform](https://console.anyscale.com/v2/) and upload the training and validation files in the [fine-tuning tab](https://console.anyscale.com/v2/fine-tuning?fine-tuning-tab=files). Make a note of the file IDs for each. This will be passed to the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYSCALE_API_KEY = \"esecret_yourKeyHere\"  # from https://console.anyscale.com/credentials\n",
    "ANYSCALE_API_BASE = \"https://api.endpoints.anyscale.com/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyscale Endpoints are OpenAI compatible\n",
    "client = openai.OpenAI(\n",
    "    base_url = ANYSCALE_API_BASE,\n",
    "    api_key = \"esecret_yourKeyHere\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now launch a fine-tuning job for 4 epochs. The expected time for this job is < 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other parameters like context length will be chosen appropriately based on dataset size\n",
    "client.fine_tuning.jobs.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    hyperparameters={\"n_epochs\": 4},\n",
    "    # replace with the actual file ids!\n",
    "    training_file=\"file_trainingFileId\",\n",
    "    validation_file=\"file_validationFileId\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Serving\n",
    "\n",
    "## Step 3(a): Finetuned on the Anyscale Platform\n",
    "\n",
    "Make a note of the final checkpoint after fine-tuning (this should be the last line in the logs). You can now spin up the \"Deploy LLMs\" template which has all the instructions and required dependencies to serve your finetuned model efficiently. You will find the tutorials on [serving LoRA models](https://github.com/anyscale/templates/blob/main/templates/endpoints_v2/examples/lora/DeployLora.ipynb) (if applicable) and on deploying a [custom model](https://github.com/anyscale/templates/blob/main/templates/endpoints_v2/examples/CustomModels.ipynb) helpful. Once you have set up your fine-tuned model as an Anyscale Service, note down the base URL and API key and place them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be run only if you finetuned on the Anyscale platform\n",
    "ANYSCALE_API_KEY=\"service-api-key-here\"\n",
    "ANYSCALE_API_BASE=\"service-url-here\" \n",
    "if not ANYSCALE_API_BASE.endswith(\"/\"):\n",
    "    ANYSCALE_API_BASE += \"/\"\n",
    "ANYSCALE_API_BASE += \"v1\"\n",
    "# enter the model id here. This would be different depending on whether you performed LoRA or full parameter fine-tuning.\n",
    "# Example: meta-llama/Meta-Llama-3-8B-Instruct:mysuffix:myid \n",
    "MODEL_ID = \"ModelIdHere\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3(b): Finetuned through serverless endpoints\n",
    "\n",
    "To serve the fine-tuned model, you just need to navigate to the \"Serving\" section on the Anyscale Platform. Your fine-tuned model should already be visible in the list of available models! \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/serving_endpoints.png\" alt=\"Serve Endpoints\">\n",
    "</p>\n",
    "\n",
    "\n",
    "As in the above image, click on the three dots and then click on \"Query\". This will provide you the starter code to interact with the model via curl, python, etc. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/serve_api_key.png\" alt=\"API Key\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is only if you finetuned through serverless endpoints\n",
    "ANYSCALE_API_BASE = \"https://api.endpoints.anyscale.com/v1\"\n",
    "ANYSCALE_API_KEY = \"esecret_yourKeyHere\"\n",
    "MODEL_ID = \"yourModelIdHere\" # make sure to not add a stray slash \"/\"\" at the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Try out the model via Playground\n",
    "\n",
    "You can try out your new model in the Playground: https://console.anyscale.com/v2/playground . In the model dropdown, you should be able to see your finetuned model as shown below\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/playground.png\" alt=\"Playground\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation\n",
    "\n",
    "Let's evaluate our trained model with GPT-4 as a baseline. \n",
    "\n",
    "\n",
    "## Evaluation strategy\n",
    "\n",
    "Evaluation of function calling capability is non-trivial, given that we're looking to extract structured data from an inherently unpredictable and unstructured stream of text. We will use the following simple evaluation strategy: The models are evaluated on the accuracy metric and their responses are graded as accurate if their response for each assistant entry in the conversation is correct. An assistant response is graded as correct under the below conditions:\n",
    "1. In case the ground truth response contains no function call, then the model's response should also not have a function call. \n",
    "2. In case the ground truth response contains a function call, then the model's response should also have a function call. We do not check for the content of the response here. The assistant function call should further have the correct function name and the correct function arguments. \n",
    "\n",
    "The following psuedocode shows some of the different branching conditions considered during evaluation:\n",
    "\n",
    "```\n",
    "if(ground_truth has no function call):\n",
    "    correct = (response has no function call)\n",
    "else\n",
    "    if response has no function call: \n",
    "        correct = False\n",
    "    else\n",
    "          if response.function_name != gt.function_name:\n",
    "                correct = False\n",
    "          else\n",
    "                correct = (response.argument_dict == gt.argument_dict)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset formatting\n",
    "We follow the same preprocessing as during training for the finetuned model hosted on Anyscale. For GPT-4, the tool calls and tool responses are parsed and formatted in the OpenAI format. \n",
    "\n",
    "We process the test dataset to have the following fields: \n",
    "1. `openai_messages` : the list of messages in the conversation for GPT-4. The user messages will be fed to the model one by one. The function responses have to be designated with the role \"tool\" here.\n",
    "2. `anyscale_messages`: the list of messages in the conversation for our fine-tuned model.\n",
    "3. `tools`: the list of tools to pass to the OpenAI model. \n",
    "4. `expected_responses` : the list of ground truth assistant responses in the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test dataset for evaluation\n",
    "modified_ds = get_evaluation_dataset(test_ds, TOOL_CALL_TAGS, TOOL_RESULT_TAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "For evaluation, we initialise two parsers - one for each model - to handle obtaining chat completions from the respective API and parsing the result. Then, our evaluation logic takes care of matching the assistant response with the expected response and, if the response is incorrect, making note of the type of error (wrong intent, wrong function name, etc). Populate the API keys below and run the below code blocks to get evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your openai key below.\n",
    "OPENAI_API_KEY = \"yourApiKeyHere\" \n",
    "# enter your Anyscale key below. If you finetuned through Anyscale endpoints, you can get the key here: https://console.anyscale.com/credentials. Otherwise, you should use the key from your Anyscale Service\n",
    "ANYSCALE_API_KEY = \"yourApiKeyHere\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parsers\n",
    "openai_parser = OpenAIResponseParser(api_key=OPENAI_API_KEY, api_base=\"https://api.openai.com/v1\", model=\"gpt-4\", tool_call_tags=TOOL_CALL_TAGS)\n",
    "anyscale_parser = AnyscaleResponseParser(api_key=ANYSCALE_API_KEY, api_base=ANYSCALE_API_BASE, model=MODEL_ID, tool_call_tags=TOOL_CALL_TAGS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# evaluate both models and plot the results\n",
    "results_gpt, accuracy_gpt = evaluate_model(modified_ds, openai_parser, \"gpt\")\n",
    "results_finetuned, accuracy_finetuned = evaluate_model(modified_ds, anyscale_parser, \"finetuned\")\n",
    "print(\"GPT-4 Accuracy: \", accuracy_gpt)\n",
    "print(\"Fine-tuned Model Accuracy: \", accuracy_finetuned)\n",
    "plot_results(results_finetuned, results_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final fine-tuned model should be able to rival GPT-4 level performance on this dataset. In fact, performance can be higher, due to the fact that the test dataset construction was straightforward. Here's how your error analysis plot might look like for `Llama-3-8B-Instruct`:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/error_analysis.png?version=1\" alt=\"Error Analysis\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congrats! You have now fine-tuned an open source model that can rival GPT-4 on function calling. As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Preprocesing a function calling dataset into a conversational format\n",
    "2. Fine-tuning a language model through either the Anyscale Platform or through Anyscale Endpoints\n",
    "3. Serving the fine-tuned model on Anyscale\n",
    "4. Evaluating the model against GPT-4 and analysing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
