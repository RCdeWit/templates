{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Services\n",
    "\n",
    "**⏱️ Time to complete**: 10 min (15 on GCP)\n",
    "\n",
    "This example shows you how to:\n",
    "1. Develop a simple Ray Serve app in a workspace.\n",
    "2. Deploy the app to production as an Anyscale Service.\n",
    "3. Monitor the production app.\n",
    "4. Configure service scaling.\n",
    "\n",
    "**Note**: This example runs in a workspace. See [Intro to Workspaces](https://docs.endpoints.anyscale.com/preview/) before running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop an app in a workspace\n",
    "\n",
    " The fastest way to develop a Ray Serve app is in an Anyscale Workspace. A Ray Serve app running within a workspace behaves almost identically to deploying a Ray Serve app to production as a service, except that it doesn't have a stable DNS name or fault tolerance.\n",
    "\n",
    " Look at the `main.py` file, which has the following skeleton code:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from fastapi import FastAPI\n",
    "from ray import serve\n",
    "\n",
    "fastapi = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(fastapi)\n",
    "class FastAPIDeployment:\n",
    "    # FastAPI automatically parses the HTTP request.\n",
    "    # See https://docs.ray.io/en/latest/serve/http-guide.html\n",
    "    @fastapi.get(\"/hello\")\n",
    "    def say_hello(self, name: str) -> str:\n",
    "        return f\"Hello {name}!\"\n",
    "\n",
    "my_app = FastAPIDeployment.bind()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the app in the workspace\n",
    "Use the command below to run the Ray Serve app in the workspace on `localhost:8000`.\n",
    "\n",
    "If you want to run it again, use the same command to update the app.\n",
    "\n",
    "**Tip**: Use `serve run main:my_app --blocking` in a new VSCode terminal to block and print out application logs (exceptions, etc.) in the terminal, allowing you to view Ray Serve application logs more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run main:my_app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a test request\n",
    "Run the following cell to query the Ray Serve app running in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.get(\"http://localhost:8000/hello\", params={\"name\": \"Theodore\"}).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to production as a service\n",
    "\n",
    "To enable fault tolerance and expose your app to the public internet, you must \"deploy\" it, which creates an Anyscale Service backed by a public load balancer. Anyscale deploys the app in a new cluster, separate from the workspace cluster. The Anyscale control plane monitors the service to recover on node failures. You can also deploy rolling updates to the service without incurring downtime.\n",
    "\n",
    "Use the following command to deploy your app as `my_service`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale service deploy main:my_app --name=my_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Overview page in the console\n",
    "\n",
    "Navigate to your newly created service in the Anyscale console at **Home > Services > my_service**. It should be in the \"Starting\" state. Click the service name and wait for the service to enter the \"Running\" state.\n",
    "\n",
    "You should see the service state, key metrics, and system event logs on the Overview page.\n",
    "\n",
    "<img src=\"assets/service-overview.png\" height=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query from the public internet\n",
    "\n",
    "Once the service is running, query the service from the public internet using similar logic from the test query in the development workspace. Make two changes:\n",
    "1. Update the `HOST` to the service endpoint.\n",
    "2. Add the authorization token as a header in the HTTP request.\n",
    "\n",
    "Find the `HOST` and authorization token values in:\n",
    "- The output of `anyscale service deploy`\n",
    "- On the service page by clicking on the **Query**\n",
    "\n",
    "For example, look for the following in the `anyscale service deploy` output: \n",
    "\n",
    "```bash\n",
    "(anyscale +4.0s) You can query the service endpoint using the curl request below:\n",
    "(anyscale +4.0s) curl -H 'Authorization: Bearer 26hTWi2kZwEz0Tdi1_CKRep4NLXbuuaSTDb3WMXK9DM' https://stable_diffusion_app-4rq8m.cld-ltw6mi8dxaebc3yf.s.anyscaleuserdata-staging.com\n",
    "```\n",
    "\n",
    "In the previous output:\n",
    "- The service endpoint value is: `https://stable_diffusion_app-4rq8m.cld-ltw6mi8dxaebc3yf.s.anyscaleuserdata-staging.com`.\n",
    "- The authorization token value is: `26hTWi2kZwEz0Tdi1_CKRep4NLXbuuaSTDb3WMXK9DM`.\n",
    "\n",
    "Replace the placeholder values in the following cell before running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "HOST = \"TODO_INSERT_YOUR_SERVICE_HOST\"\n",
    "TOKEN = \"TODO_INSERT_YOUR_SERVICE_TOKEN\"\n",
    "\n",
    "def send_request(name: str) -> str:\n",
    "    response: requests.Response = requests.get(\n",
    "        f\"{HOST}/hello\",\n",
    "        params={\"name\": name},\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        },\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(send_request(\"Theodore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor production services\n",
    "\n",
    "Along with the monitoring tools that come with workspaces, services provide additional built-in metrics that you can find in the `Metrics` tab. This tab includes aggregated metrics across all rollouts for a service, possibly from multiple clusters.\n",
    "\n",
    "<img src=\"assets/service-metrics.png\" height=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure service scaling\n",
    "\n",
    "By default, a service has a single replica. To change this configuration, set the `num_replicas` argument in the [serve.deployment decorator](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html) as follows in `main.py`:\n",
    "\n",
    "```python\n",
    "@serve.deployment(num_replicas=4)\n",
    "@serve.ingress(fastapi)\n",
    "class FastAPIDeployment:\n",
    "    ...\n",
    "```\n",
    "\n",
    " For more advanced scaling options, see [Ray Serve Autoscaling](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling).\n",
    " \n",
    "Rerun the service in the development workspace using `serve run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run main:my_app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the increase in the number of replicas in the Ray Dashboard of the development workspace:\n",
    "\n",
    "<img src=\"assets/serve-replicas.png\" height=400px/>\n",
    "\n",
    "On the production service, deploy the update, making sure to include the `--name` option to specify which service to deploy to. This command triggers a staged rollout of the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale service deploy main:my_app --name=my_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the status of the rollout in the service Overview page. Once the new cluster with the updated app config is running, Ray Serve shuts down the previous cluster:\n",
    "\n",
    "<img src=\"assets/service-rollout.png\" height=300px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Serve autoscaling config vs compute config\n",
    "\n",
    " During service scaling, the Ray Serve autoscaling config interacts with the compute config of the cluster. The `@serve.deployment` decorater contains the autoscaling config, such as `num_replicas`, and the compute config sets the number of worker nodes. Generally, the compute config is an upper bound on service scaling because Ray Serve runs inside the cluster. For example, if you configure the cluster to have at most 100 CPUs, then Ray Serve can only launch up to 100 replicas, regardless of the autoscaling config.\n",
    "\n",
    "For this reason, enable the \"Auto-select machines\" compute config for services. This setting is on by default.\n",
    "\n",
    "#### Edit a compute config\n",
    "\n",
    "When Anyscale first creates a service, it copies the compute config from the workspace. After that, Anyscale decouples the service cluster's compute config from the workspace and you can edit it independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more, try other model serving templates available in the template gallery, and the Ray Serve [documentation](https://docs.ray.io/en/latest/serve/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you:\n",
    "- Developed and ran a simple Ray Serve app in a development workspace.\n",
    "- Deployed the app to production as a service.\n",
    "- Monitored the service.\n",
    "- Scaled the service that uses both the Ray Serve autoscaling config and compute config together."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
