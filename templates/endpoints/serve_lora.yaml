applications:
  - name: meta-llama--Llama-2-7b-chat-hf
    route_prefix: /
    import_path: aviary_private_endpoints.backend.server.run:router_application
    args:
      models: []
      multiplex_models:
        - "./models/meta-llama--Llama-2-7b-chat-hf_a10.yaml"
      multiplex_lora_adapters:  # <-- this is the loRA configs you can use to set up fixed loRA
        - "./multiplex_lora_adapters/static_lora_adapter_config.yaml"
      dynamic_lora_loading_path: s3://my-bucket/my-lora-checkouts/  # <-- change this to a bucket you have access to that contains the loRA checkpoint to load dynamically
    runtime_env:
      env_vars:
        HUGGING_FACE_HUB_TOKEN: "UPDATE HF TOKEN HERE"  # <-- change this to a real token
