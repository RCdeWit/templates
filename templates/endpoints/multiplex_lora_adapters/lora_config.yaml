base_model_id: meta-llama/Llama-2-7b-chat-hf  # <-- this is the base model for loRA
model_id: lora-viggo-finetuned  # <-- this is the loRA model you can use to run query
generation:
  prompt_format:
    system: "<<SYS>>\n{instruction}\n<</SYS>>\n\n"
    assistant: " {instruction} </s><s> "
    trailing_assistant: " "
    user: "[INST] {system}{instruction} [/INST]"
    system_in_user: true
    default_system_message: ""
  stopping_sequences: ["<unk>"]
lora_mirror_config:
  bucket_uri: s3://your-own-model-bucket/lora-checkpoint-path/  # <-- change this to a bucket you have access to that contains the loRA checkpoint
