{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Triton Inference Server on Anyscale Services\n",
    "\n",
    "**⏱️ Time to complete**: 30 minutes (1 hour on GCP)\n",
    "\n",
    "This guide walks you through develop and deploy Triton Server applications in Anyscale\n",
    "though running a Stable Diffusion 1.5 service. \n",
    "\n",
    "In this tutorial, you will learn:\n",
    "1. Build Docker image for Triton Server to run in Anyscale platform.\n",
    "2. Compile model using Triton's Python backend on Anyscale Workspaces.\n",
    "3. Run Triton Server locally on Anyscale Workspaces. \n",
    "4. Deploy the application on Anyscale Services.\n",
    "\n",
    "**Note**: This guide is not meant to substitute with the official Triton documentation.\n",
    "For more information, please refer to the\n",
    "[Triton Inference Server documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html).\n",
    "\n",
    "## Build Docker image for Triton Server to run in Anyscale platform\n",
    "\n",
    "Let's start by building a Docker image that can run properly in Anyscale platform. You\n",
    "do not need to build the image from scratch. The image used to run this tutorial is\n",
    "already used in the workspace. This section is just for informational purposes.\n",
    "Some of the code used here are taken piecewise from Nvidia's tutorials from [TenserRT Stable Diffusion](https://github.com/NVIDIA/TensorRT/blob/release/10.0/demo/Diffusion/README.md),\n",
    "[Triton building complex pipeline](https://github.com/triton-inference-server/tutorials/blob/r24.04/Conceptual_Guide/Part_6-building_complex_pipelines/README.md),\n",
    "and [Triton Ray Serve Deployment](https://github.com/triton-inference-server/tutorials/tree/r24.04/Triton_Inference_Server_Python_API/examples/rayserve).\n",
    "If you need to learn more on TensorRT and Triton, please refer to their official\n",
    "documentation and tutorial.\n",
    "\n",
    "In Nvidia's tutorial, they have an image that is used to build the model and\n",
    "another image that is used to serve the model. In this tutorial, we will pull in all the\n",
    "necessary dependencies into one image. This image will be used to compile the model, \n",
    "do local development in the Anyscale Workspace, and deploy to Anyscale Services.\n",
    "\n",
    "The full `Dockerfile` used to build the image is already provided in the workspace.\n",
    "You can view the file by clicking on the `Dockerfile` in the file explorer. It starts\n",
    "with Nvidia's Triton serve base image `nvcr.io/nvidia/tritonserver:24.04-py3`. This\n",
    "can be updated to the later version depending on when you are running this tutorial in\n",
    "the future.\n",
    "\n",
    "```Dockerfile\n",
    "FROM nvcr.io/nvidia/tritonserver:24.04-py3\n",
    "```\n",
    "\n",
    "Then it installs the necessary for dependencies for compile and serve Stable Diffusion\n",
    "models.\n",
    "\n",
    "```Dockerfile\n",
    "RUN pip install --no-cache-dir tritonclient[all]==2.45.0 torch==2.3.0 diffusers==0.23.1 \\\n",
    "    onnx==1.14.0 onnx-graphsurgeon==0.5.2 polygraphy==0.49.9 transformers==4.31.0 scipy==1.13.0 \\\n",
    "    nvtx==0.2.10 accelerate==0.30.1 optimum[onnxruntime]==1.19.2 nvidia-ammo==0.9.4\n",
    "```\n",
    "\n",
    "The next section installs Triton's Python API and pulls in `model.py` and `config.pbtxt`\n",
    "from `triton-inference-server/tutorials` repository to serve Stable Diffusion 1.5 model.\n",
    "\n",
    "```Dockerfile\n",
    "RUN git clone https://github.com/triton-inference-server/tutorials.git -b r24.04 --single-branch /tmp/tutorials\n",
    "RUN pip --no-cache-dir install /tmp/tutorials/Triton_Inference_Server_Python_API/deps/tritonserver-2.41.0.dev0-py3-none-any.whl[all]\n",
    "RUN mkdir -p /opt/tritonserver/backends/diffusion\n",
    "RUN cp /tmp/tutorials/Popular_Models_Guide/StableDiffusion/backend/diffusion/model.py /opt/tritonserver/backends/diffusion/model.py\n",
    "RUN mkdir -p /tmp/workspace/diffusion-models/stable_diffusion_1_5/1\n",
    "RUN cp /tmp/tutorials/Popular_Models_Guide/StableDiffusion/diffusion-models/stable_diffusion_1_5/config.pbtxt /tmp/workspace/diffusion-models/stable_diffusion_1_5/config.pbtxt\n",
    "```\n",
    "\n",
    "Then it installs TensorRT and copy the Diffusion backend to be used in Triton. Also\n",
    "note the backend code consists of loading the Stable Diffusion model, compile into\n",
    "TensorRT, and store the TensorRT to Triton's model repository. If you are interested\n",
    "how that works, you can take a look at the backend diffusion directory.\n",
    "\n",
    "```Dockerfile\n",
    "RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com tensorrt==10.0.1\n",
    "RUN git clone https://github.com/NVIDIA/TensorRT.git -b v10.0.1 --single-branch /tmp/TensorRT\n",
    "RUN cp -rf /tmp/TensorRT/demo/Diffusion /opt/tritonserver/backends/diffusion/\n",
    "```\n",
    "\n",
    "All the above steps are specifically for building and serving the Stable Diffusion 1.5\n",
    "model. If you are working on different models, you can change them accordingly.\n",
    "\n",
    "The last section of the `Dockerfile` is to set up Anyscale related dependencies for it\n",
    "to run on the Anyscale platform. You can keep this section as is regardless which model\n",
    "you are working with.\n",
    "\n",
    "## Compile model using Triton's Python backend on Anyscale Workspaces\n",
    "\n",
    "Unlike Nvidia's tutorial, we do not include models in the Docker image. Instead, in this\n",
    "section we will build the model and upload the model to a cloud storage such as AWS S3\n",
    "or GCP Cloud Storage for serving later.\n",
    "\n",
    "Run this code to start a Triton server using the tmp directory as the model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonserver\n",
    "\n",
    "model_repository = [\"/tmp/workspace/diffusion-models\"]\n",
    "\n",
    "triton_server = tritonserver.Server(\n",
    "    model_repository=model_repository,\n",
    "    model_control_mode=tritonserver.ModelControlMode.EXPLICIT,\n",
    ")\n",
    "triton_server.start(wait_until_ready=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model using Triton's Python backend. This will take 10-15 minutes on a\n",
    "T4 GPU and 8-10 minutes on an A10 GPU. The model will be compiled and saved in the\n",
    "`model_repository` directory as TensorRT engine. Also keep in mind the model has to be\n",
    "built in the same type of GPU you are planning to serve the model on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "print(f\"start time: {datetime.datetime.now()}\")\n",
    "t0 = time.time()\n",
    "model = triton_server.load(\"stable_diffusion_1_5\")\n",
    "duration = time.time() - t0\n",
    "print(f\"Total duration: {duration}s\")\n",
    "\n",
    "# Unload the model and the server to free the memory.\n",
    "triton_server.unload(model, wait_until_unloaded=True)\n",
    "triton_server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is compiled, we can upload the model to a cloud storage. We need to\n",
    "upload both the model config file `config.pbtxt` and the TensorRT engine model\n",
    "directory. Anyscale provides a environment variable `ANYSCALE_ARTIFACT_STORAGE` that\n",
    "can be used to store model artifacts. Use one of the following to upload the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If running in AWS\n",
    "!aws s3 cp /tmp/workspace/diffusion-models/stable_diffusion_1_5/config.pbtxt $ANYSCALE_ARTIFACT_STORAGE/triton_model_repository/stable_diffusion_1_5/config.pbtxt\n",
    "!aws s3 cp /tmp/workspace/diffusion-models/stable_diffusion_1_5/1/1.5-engine-batch-size-1/ $ANYSCALE_ARTIFACT_STORAGE/triton_model_repository/stable_diffusion_1_5/1/1.5-engine-batch-size-1/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in GCP\n",
    "!gcloud storage cp /tmp/workspace/diffusion-models/stable_diffusion_1_5/config.pbtxt $ANYSCALE_ARTIFACT_STORAGE/triton_model_repository/stable_diffusion_1_5/config.pbtxt\n",
    "!gcloud storage cp /tmp/workspace/diffusion-models/stable_diffusion_1_5/1/1.5-engine-batch-size-1/ $ANYSCALE_ARTIFACT_STORAGE/triton_model_repository/stable_diffusion_1_5/1/1.5-engine-batch-size-1/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Triton Server locally on Anyscale Workspaces\n",
    "\n",
    "The `Deployment.py` is included in this workspace for you. This file demonstrates using\n",
    "remote model repository to start triton serve, loading the specific Stable Diffusion\n",
    "model, run inference with Triton, and serving the response through Ray Serve.\n",
    "You can view the file by clicking on the `Deployment.py` in the file explorer. In\n",
    "addition, you can also do prompt engineering, apply business logics, or doing model\n",
    "composition with Ray Serve before returning the response as image. Run the follow code\n",
    "to start Triton Server with Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!serve run deployment:triton_deployment --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GPU worker node will be started by autoscaler. The model will be downloaded from the\n",
    "cloud storage location where we just uploaded the model artifacts, and then loaded into\n",
    "the Triton Server and serve the endpoint via Ray Serve. It might take few minutes to\n",
    "start the server. \n",
    "\n",
    "Once you see the message \"Deployed app 'default' successfully.\". You can run the\n",
    "following command to query the endpoint and save the image to a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl \"http://localhost:8000/generate?prompt=dogs%20in%20new%20york,%20realistic,%204k,%20photograph\" > dogs_photo.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of generated image will look like the following\n",
    "\n",
    "<img src=\"assets/dogs_photo.jpg\"/>\n",
    "\n",
    "## Deploy the application on Anyscale Services\n",
    "\n",
    "The `config.yaml` is also included in the workspace for you. You can view the file by\n",
    "clicking on the `config.yaml` in the file explorer. Once you completed local\n",
    "development on the workspace and ready to move to production, you can deploy the\n",
    "service onto Anyscale Services by running the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!anyscale service deploy -f config.yaml --name \"triton-stable-diffusion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will start a fresh cluster, and deploy the same code you just ran locally\n",
    "to a service. The cluster should contain two nodes, a head node, and a worker node,\n",
    "where the replica will be running on the worker. You will see output like the following\n",
    "\n",
    "```commandline\n",
    "(anyscale +1.5s) Starting new service 'triton-stable-diffusion'.\n",
    "(anyscale +2.1s) Uploading local dir '.' to cloud storage.\n",
    "(anyscale +2.9s) Including workspace-managed pip dependencies.\n",
    "(anyscale +3.8s) Service 'triton-stable-diffusion' deployed.\n",
    "(anyscale +3.8s) View the service in the UI: 'https://console.anyscale.com/services/service2_s8cwtlwwvukzxzd256z1wyqmj9'\n",
    "(anyscale +3.8s) Query the service once it's running using the following curl command:\n",
    "(anyscale +3.8s) curl -H 'Authorization: Bearer pnnHyxUG_v6hzLbUn7LLmgNjF5g3t0XAxa0TXoRFV6g' https://triton-stable-diffusion-bxauk.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/\n",
    "```\n",
    "\n",
    "You can click on the link to the services UI to check the status. Once it's in the\n",
    "running status, you can run the following command to test the endpoint. Make sure to\n",
    "change the bearer token and the base URL to the one showed from the above deploy output.\n",
    "This command will query against the newly deployed service and store the generated image\n",
    "locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -H \"Authorization: Bearer pnnHyxUG_v6hzLbUn7LLmgNjF5g3t0XAxa0TXoRFV6g\" \\\n",
    "    \"https://triton-stable-diffusion-bxauk.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/generate?prompt=dogs%20in%20new%20york,%20realistic,%204k,%20photograph\" \\\n",
    "    > dogs_photo_service.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "An example of generated image will look like the following\n",
    "\n",
    "<img src=\"assets/dogs_photo_service.jpg\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
