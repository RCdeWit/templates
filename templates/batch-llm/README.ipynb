{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM offline batch inference with Ray Data and vLLM\n",
    "\n",
    "**⏱️ Time to complete**: 10 min\n",
    "\n",
    "This template shows you how to:\n",
    "1. Read in data from in-memory samples or files on cloud storage. \n",
    "2. Use Ray Data and vLLM to run batch inference of a LLM.\n",
    "3. Write the inference outputs to cloud storage.\n",
    "\n",
    "For a Python script version of the `.ipynb` notebook used for the workspace template, refer to `examples/main.py`.\n",
    "\n",
    "**Note:** This tutorial is run within a workspace. Please overview the `Introduction to Workspaces` template first before this tutorial.\n",
    "\n",
    "### How to decide between online vs offline inference for LLM\n",
    "Online LLM inference (e.g. Anyscale Endpoint) should be used when you want to get real-time response for prompt or to interact with the LLM. Use online inference when you want to optimize latency of inference to be as quick as possible.\n",
    "\n",
    "On the other hand, offline LLM inference should be used when you want to get reponses for a large number of prompts within some time frame, but not required to be real-time (minutes to hours granularity). Use offline inference when you want to:\n",
    "1. Scale your workload to large-scale datasets\n",
    "2. optimize inference throughput and resource usage (for example, maximizing GPU utilization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Python dependencies\n",
    "Install additional required dependencies using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum transformers version compatible with Mixtral models.\n",
    "!pip install -q vllm==0.3.3 transformers>=4.38.0 && echo 'Install complete!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the dependencies used in this template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from util.utils import generate_output_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up model defaults\n",
    "Set up default values that will be used in the batch inference workflow:\n",
    "* Your [Hugging Face user access token](https://huggingface.co/docs/hub/en/security-tokens). This will be used to download the model.\n",
    "* The model to use for inference ([see the list of vLLM models](https://docs.vllm.ai/en/latest/models/supported_models.html)).\n",
    "    * This workspace template has been tested and verified with the following models:\n",
    "        * `meta-llama/Llama-2-7b-chat-hf`\n",
    "        * `mistralai/Mistral-7B-Instruct-v0.1`\n",
    "        * `google/gemma-7b-it`\n",
    "        * `mlabonne/NeuralHermes-2.5-Mistral-7B`\n",
    "    * Support for the following larger models are actively a work-in-progress, and will be supported very soon:\n",
    "        * `meta-llama/Llama-2-13b-chat-hf`\n",
    "        * `mistralai/Mixtral-8x7B-Instruct-v0.1`\n",
    "        * `meta-llama/Llama-2-70b-chat-hf`\n",
    "        * `codellama/CodeLlama-70b-Instruct-hf`\n",
    "\n",
    "* The [sampling parameters object](https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py) used by vLLM.\n",
    "* The output path where results will be written as parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hugging Face token. Replace the following with your token.\n",
    "HF_TOKEN = \"<REPLACE_WITH_YOUR_HUGGING_FACE_USER_TOKEN>\"\n",
    "# Set to the model that you wish to use from the preceding list.\n",
    "# Note that using the Llama models will require a Hugging Face token to be set.\n",
    "HF_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=2048)\n",
    "# Output path to write output result. You can also change this to any cloud storage path,\n",
    "# e.g. a specific S3 bucket.\n",
    "output_path = generate_output_path(os.environ.get(\"ANYSCALE_ARTIFACT_STORAGE\"), HF_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray, using the Hugging Face token as an environment variable so that it's made available to all nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\"HF_TOKEN\": HF_TOKEN},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read input data with Ray Data\n",
    "Use Ray Data to read in your input data from some sample prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample prompts, and use Ray Data to create a dataset for it.\n",
    "prompts = [\n",
    "    \"I always wanted to be a ...\",\n",
    "    \"The best way to learn a new language is ...\",\n",
    "    \"The biggest challenge facing our society today is ...\",\n",
    "    \"One thing I would change about my past is ...\",\n",
    "    \"The key to a happy life is ...\",\n",
    "]\n",
    "ds = ray.data.from_items(prompts)\n",
    "\n",
    "# View one row of the Dataset.\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Batch Inference with vLLM\n",
    "\n",
    "Create a class to define batch inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of model name to max_model_len supported by model.\n",
    "model_name_to_args = {\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\": {\"max_model_len\": 16832},\n",
    "    \"google/gemma-7b-it\": {\"max_model_len\": 2432},\n",
    "    \"mlabonne/NeuralHermes-2.5-Mistral-7B\": {\"max_model_len\": 16800},\n",
    "}\n",
    "\n",
    "class LLMPredictor:\n",
    "    def __init__(self, text_column):\n",
    "        # Name of column containing the input text.\n",
    "        self.text_column = text_column\n",
    "\n",
    "        # Create an LLM.\n",
    "        self.llm = LLM(\n",
    "            model=HF_MODEL,\n",
    "            **model_name_to_args.get(HF_MODEL, {}),\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "        # Generate texts from the prompts.\n",
    "        # The output is a list of RequestOutput objects that contain the prompt,\n",
    "        # generated text, and other information.\n",
    "        outputs = self.llm.generate(batch[self.text_column], sampling_params)\n",
    "        prompt = []\n",
    "        generated_text = []\n",
    "        for output in outputs:\n",
    "            prompt.append(output.prompt)\n",
    "            generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with GPUs\n",
    "\n",
    "Apply batch inference for all input data with the Ray Data [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) method. When using vLLM, LLM instances require GPUs; here, we will demonstrate how to configure Ray Data to scale the number of LLM instances and GPUs needed.\n",
    "\n",
    "To use GPUs for inference in the Workspace, we can specify `num_gpus` and `concurrency` in the `ds.map_batches()` call below to indicate the number of LLM instances and the number of GPUs per LLM instance, respectively. For example, with `concurrency=4` and `num_gpus=1`, we have 4 LLM instances, each using 1 GPU, so we need 4 GPUs total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    # Set the concurrency to the number of LLM instances.\n",
    "    concurrency=4,\n",
    "    # Specify the number of GPUs required per LLM instance.\n",
    "    num_gpus=1,\n",
    "    # Specify the batch size for inference. Set the batch size to as large possible without running out of memory.\n",
    "    # If you encounter out-of-memory errors, decreasing batch_size may help.\n",
    "    batch_size=10,\n",
    "    # Pass keyword arguments for the LLMPredictor class.\n",
    "    fn_constructor_kwargs={\"text_column\": \"item\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make sure to either enable *Auto-select worker nodes* or configure your workspace cluster to have the appropriate GPU worker nodes:\n",
    "\n",
    "![title](assets/ray-data-gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to start dataset execution and view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to a larger dataset\n",
    "In the example above, we performed batch inference for Ray Dataset with 5 example prompts. Next, let's explore how to scale to a larger dataset based on files stored in cloud storage.\n",
    "\n",
    "Run the following cell to create a Dataset from a text file stored on S3. This Dataset has 100 rows, with each row containing a single prompt in the `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_text(\"s3://anonymous@air-example-data/prompts_100.txt\")\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we apply batch inference for all input data with the Ray Data [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    # Set the concurrency to the number of LLM instances.\n",
    "    concurrency=4,\n",
    "    # Specify the number of GPUs required per LLM instance.\n",
    "    num_gpus=1,\n",
    "    # Specify the batch size for inference. Set the batch size to as large possible without running out of memory.\n",
    "    # If you encounter CUDA out-of-memory errors, decreasing batch_size may help.\n",
    "    batch_size=10,\n",
    "    # Pass keyword arguments for the LLMPredictor class.\n",
    "    fn_constructor_kwargs={\"text_column\": \"text\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to execute and view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Ray Dashboard tab, navigate to the Job page and open the \"Ray Data Overview\" section to view the details of the batch inference execution:\n",
    "\n",
    "![title](assets/ray-data-jobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling GPU out-of-memory failures\n",
    "If you run into CUDA out of memory, your batch size is likely too large. Decrease the batch size as described above.\n",
    "\n",
    "If your batch size is already set to 1, then use either a smaller model or GPU devices with more memory.\n",
    "\n",
    "For advanced users working with large models, you can use model parallelism to shard the model across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Results\n",
    "Finally, write the inference output data out to Parquet files on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write_parquet(output_path)\n",
    "print(f\"Batch inference result is written into {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Ray Data to read back the output files to ensure the results are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_output = ray.data.read_parquet(output_path)\n",
    "ds_output.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting an Anyscale Job\n",
    "\n",
    "Now that we have successfully created this simple Ray task, let's convert it into an Anyscale Job. We can submit the Anyscale job using the Python version of this example (`examples/main.py`). Anyscale Jobs are the recommended way to run workloads (such as data processing, model training, or fine-tuning) in production. You can learn more about Anyscale Jobs [here](https://docs.anyscale.com/productionize/jobs/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale job submit -- python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- Read in data from in-memory samples or input files from cloud storage. \n",
    "- Used Ray Data and vLLM to run offline batch inference of a LLM.\n",
    "- Wrote the inference outputs to cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 25 2022, 14:13:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
