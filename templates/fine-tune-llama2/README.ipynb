{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama-2 models with Deepspeed, Accelerate, and Ray Train\n",
    "\n",
    "**⏱️ Time to complete**: 42 min\n",
    "\n",
    "This template shows you how to fine-tune Llama-2 models. \n",
    "\n",
    "### Step 1: Fine-tune Llama-2-7B model\n",
    "- Run the command below to kick off fine-tuning with dummy data, [Grade School Math 8k (GSM8K) dataset](https://huggingface.co/datasets/gsm8k).\n",
    "\n",
    "The flag `--as-test` is for testing purpose as it runs through only one forward and backward pass and checkpoints the model. It takes ~7 mins. Without this flag, it takes ~42 mins (3 epochs for optimal model quality).\n",
    "\n",
    "Model checkpoints will be stored under `{user's first name}/ft_llms_with_deepspeed/` in the cloud storage bucket created for your Anyscale account. The full path will be printed in the output after the training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --size=7b --as-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 2: Switch to a different model size\n",
    "- Change model size (7b, 13b, or 70b) with the `--size` option in the previous command\n",
    "- Change the worker node type accordingly to fit the model\n",
    "    - use g5.12xlarge for 13b\n",
    "    - use g5.48xlarge for 70b\n",
    "\n",
    "<img src=\"https://github.com/anyscale/templates/blob/main/templates/fine-tune-llama2/assets/edit_nodes.png?raw=true\" alt=\"edit nodes\" width=\"250\"/>\n",
    "\n",
    "- Run the command below to kick off fine-tuning with new model size and worker nodes.\n",
    "    - 13b: ~9 mins for test run. ~60 mins for a full run (3 epochs)\n",
    "    - 13b: ~35 mins for test run. ~400 mins for a full run (3 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --size=13b --as-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Use your custom data\n",
    "- Replace the contents in `./data/train.jsonl` with your own training data\n",
    "- (Optional) Replace the contents in `./data/test.jsonl` with your own test data if any.\n",
    "- (Optional) Add special token in `./data/tokens.json` if any.\n",
    "\n",
    "Use the same command to train with your own data.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "## What's next.   \n",
    "\n",
    "Voila! You have fine-tuned your own Llama-2 models. Want more than this? Check out advanced tutorials below \n",
    "\n",
    "- Walkthrough of this template: navigate to `tutorials/walkthrough.md`\n",
    "- Fine-tune Llama-2 with LoRA adapters: navigate to `tutorials/lora.md`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
