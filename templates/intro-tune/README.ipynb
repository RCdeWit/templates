{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Experiments in Parallel with Tune\n",
    "\n",
    "Ray Tune lets you easily run experiments in parallel across a cluster.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "1. How to set up a Ray Tune app to run an parallel grid sweep across a cluster.\n",
    "2. Basic Ray Tune features, including stats reporting and storing results.\n",
    "3. Monitoring cluster parallelism and execution using the Ray dashboard.\n",
    "\n",
    "**Note**: This tutorial is run within a workspace. Please overview the ``Introduction to Workspaces`` template first before this tutorial.\n",
    "\n",
    "## Grid search hello world\n",
    "\n",
    "Let's start by running a quick \"hello world\" that runs a few variations of a function call across a cluster. It should take about 10 seconds to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "def f(config):\n",
    "    print(\"hello world from variant\", config[\"x\"])\n",
    "    return {\"my_result_metric\": config[\"x\"] ** 2}\n",
    "\n",
    "tuner = tune.Tuner(f, param_space={\"x\": tune.grid_search([0, 1, 2, 3, 4])})\n",
    "results = tuner.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "You should see during the run a table of the trials created by Tune. One trial is created for each individual value of `x` in the grid sweep. The table shows where the trial was run in the cluster, how long the trial took, and reported metrics:\n",
    "\n",
    "<img src=\"assets/tune-status.png\" width=800px/>\n",
    "\n",
    "On completion, it returns a `ResultGrid` object that captures the experiment results. This includes the reported trial metrics, the path where trial results are saved:\n",
    "\n",
    "```py\n",
    "ResultGrid<[\n",
    "  Result(\n",
    "    metrics={'my_result_metric': 0},\n",
    "    path='/home/ray/ray_results/f_2024-02-27_11-40-53/f_1e2c4_00000_0_x=0_2024-02-27_11-40-56',\n",
    "    filesystem='local',\n",
    "    checkpoint=None\n",
    "  ),\n",
    "  ...\n",
    "```\n",
    "\n",
    " Note that the filesystem of the result says \"local\", which means results are written to the workspace local disk. We'll cover how to configure [Tune storage](https://docs.ray.io/en/latest/tune/tutorials/tune-storage.html) for a distributed run later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing trial outputs\n",
    "\n",
    "To view the stdout and stderr of the trial, use the ``Logs`` tab in the Workspace UI. Navigate to the log page and search for \"hello\", and you'll be able to see the logs printed for each trial run in the cluster:\n",
    "\n",
    "<img src=\"assets/tune-logs.png\" width=800px/>\n",
    "\n",
    "Tune also saves a number of input and output metadata files for each trial to storage, you can view them by querying the returned result object:\n",
    "- ``params.json``: The input parameters of the trial\n",
    "    - ``params.pkl`` pickle form of the parameters (for non-JSON objects)\n",
    "- ``result.json``: Log of intermediate and final reported metrics\n",
    "    - ``progress.csv``: CSV form of the results\n",
    "    - ``events.out.tfevents``: TensorBoard form of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Print the list of metadata files from trial 0 of the previous run.\n",
    "os.listdir(results[0].path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring a larger-scale run\n",
    "\n",
    "Next, we'll configure Tune for a larger-scale run on a multi-node cluster. We'll customize the following parameters:\n",
    "- Resources to request for each trial\n",
    "- Saving results to cloud storage\n",
    "\n",
    "The code below walks through how to do this in Tune. Go ahead and run the cell, it will take a few minutes to complete on a multi-node cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune, train\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Do a large scale run with 100 trials, each of which takes 60 seconds to run\n",
    "# and requests two CPU slots from Ray.\n",
    "# For example, each trial could be training a variation of a model.\n",
    "NUM_TRIALS = 100\n",
    "TIME_PER_TRIAL = 60\n",
    "CPUS_PER_TRIAL = 2\n",
    "\n",
    "# Define where results are stored. We'll use the Anyscale artifact storage path to\n",
    "# save results to cloud storage.\n",
    "STORAGE_PATH = os.environ[\"ANYSCALE_ARTIFACT_STORAGE\"] + \"/tune_results\"\n",
    "\n",
    "def f(config):\n",
    "    # Import model libraries, etc...\n",
    "    # Load data and train model code here...\n",
    "    time.sleep(TIME_PER_TRIAL)\n",
    "\n",
    "    # Return final stats. You can also return intermediate progress\n",
    "    # using ray.train.report() if needed.\n",
    "    # To return your model, you could write it to storage and return its\n",
    "    # URI in this dict, or return it as a Tune Checkpoint:\n",
    "    # https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html\n",
    "    return {\"my_result_metric\": config[\"x\"] ** 2, \"other_data\": ...}\n",
    "\n",
    "# Define trial parameters as a single grid sweep.\n",
    "trial_space = {\n",
    "    # This is an example parameter. You could replace it with filesystem paths,\n",
    "    # model types, or even full nested Python dicts of model configurations, etc.,\n",
    "    # that enumerate the set of trials to run.\n",
    "    \"x\": tune.grid_search(range(NUM_TRIALS)),\n",
    "}\n",
    "\n",
    "# Can customize resources per trial, including CPUs and GPUs.\n",
    "f_wrapped = tune.with_resources(f, {\"cpu\": CPUS_PER_TRIAL})\n",
    "\n",
    "# Start a Tune run and print the output.\n",
    "tuner = tune.Tuner(\n",
    "    f_wrapped,\n",
    "    param_space=trial_space,\n",
    "    run_config=train.RunConfig(storage_path=STORAGE_PATH),\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Tune execution in the cluster\n",
    "\n",
    "Let's observe how the above run executed in the Ray cluster for the workspace. To do this, go to the \"Ray Dashboard\" tab in the workspace UI.\n",
    "\n",
    "First, let's view the run in the Jobs sub-tab and click through to into the job view. Here, you can see an overview of the job, and the status of the individual actors Tune has launched to parallelize the job:\n",
    "\n",
    "<img src=\"assets/tune-jobs-1.png\" width=800px/>\n",
    "\n",
    "You can further click through to the actors sub-page and view the status of individual running actors. Inspect trial logs, CPU profiles, and memory profiles using this page:\n",
    "\n",
    "<img src=\"assets/tune-jobs-2.png\" width=800px/>\n",
    "\n",
    "Finally, we can observe the holistic execution of the job in the cluster in the Metrics sub-tab. When running the above job on a 36-CPU cluster, we can see that Tune was able to launch ~16 concurrent actors for trial execution, with each actor assigned 2 CPU slots as configured:\n",
    "\n",
    "<img src=\"assets/tune-metrics.png\" width=800px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes our overview of Ray Tune in Anyscale. To learn more about advanced features of Tune and how it can improve your experiment management lifecycle, check out the [Ray Tune docs](https://docs.ray.io/en/latest/tune/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- Run a basic parallel grid sweep experiment in a workspace.\n",
    "- Showed how to configure Ray Tune's storage and scheduling options.\n",
    "- Demoed how to debug an experiment run in the cluster using observability tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
