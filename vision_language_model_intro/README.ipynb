{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy, configure, and serve LLMs \n",
    "\n",
    "**‚è±Ô∏è Time to complete**: 10 min\n",
    "\n",
    "Ready for some adventure into multi-modal models? Vision language model support is now added to RayLLM. The current support includes LlaVA-NeXT models with dynamic image resolution. Meta AI‚Äôs Chameleon, with its early fusion architecture, will come soon. \n",
    "\n",
    "In this example, we will use RayLLM to serve [llava-hf/llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf). We will build a Gradio application that brings art to life with ekphrasis, specially designed for kids. Imagine turning kid drawings into whimsical verses and delightful rhymes. Now let‚Äôs buckle up and dive into the fun! üé®üöÄ\n",
    "\n",
    "**Note**: This guide is hosted within an Anyscale workspace, which provides easy access to compute resources. Check out the `Introduction to Workspaces` template for more details.\n",
    "\n",
    "## Step 1 - Run the model locally in the Workspace\n",
    "\n",
    "We provide a starter command to run vision language models via Ray Serve. To generate the configuration file, run the following command directly in your terminal:\n",
    "```\n",
    "python generate_config.py\n",
    "```\n",
    "\n",
    "**Note:** This command requires interactive inputs and should be executed directly in the terminal, not within a Jupyter notebook cell.\n",
    "\n",
    "The command will generate 2 files - a model config file (saved in `model_config/`) and a serve config file (`serve_TIMESTAMP.yaml`) that you can reference and re-run in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't start the serve application in the previous step, you can start it using the following command (replace the file name with the generated `serve_` file name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_TIMESTAMP.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Query the model\n",
    "\n",
    "### Example data to query with\n",
    "\n",
    "Run the following command to download a few example kid drawings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The download path is under `/mnt/local_storage`, an NVME backed file system configured by Anyscale workspace for fast data access.\n",
    "\n",
    "To inspect the downloaded images in the workspace, the easiest way is to start a local http server using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m http.server 8080 --directory /mnt/local_storage/kid_drawings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyscale workspace has port forwarding conveniently configured. Navigate to the ‚Äúport‚Äù tab and click the corresponding url. This will open a local web browser that directly talks to, in our case, the python http server that serves the example images.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/where_ports.png\" alt=\"where ports\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/ports_list.png\" alt=\"ports list\">\n",
    "</p>\n",
    "\n",
    "Here are some example kid drawings.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/example_1.jpeg\" alt=\"Image 1\" width=\"250\" style=\"margin-right: 10px;\">\n",
    "  <img src=\"./assets/example_2.jpeg\" alt=\"Image 2\" width=\"250\" style=\"margin-right: 10px;\">\n",
    "  <img src=\"./assets/example_3.jpeg\" alt=\"Image 3\" width=\"250\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "\n",
    "You can use the OpenAI SDK to interact with the models, ensuring an easy integration for your applications.\n",
    "Specifically for vision language models, images can be passed in using either image url or based64 encoded string.\n",
    "Notice that both scripts query in a streaming fashion.\n",
    "\n",
    "**Note:** LLaVA-NeXT supports only single image and single user message for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query with image url\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def query(base_url: str, api_key: str):\n",
    "\n",
    "   client = OpenAI(\n",
    "     base_url=base_url,\n",
    "     api_key=api_key,\n",
    "   )\n",
    "   chat_completions = client.chat.completions.create(\n",
    "       model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "       messages=[\n",
    "           {\"role\": \"user\", \"content\": [\n",
    "               {\"type\": \"text\", \"text\": \"Write me a poetry for kid based on this image.\"},\n",
    "               {\"type\": \"image_url\", \"image_url\": {\n",
    "                   \"url\": \"https://air-example-data-2.s3.amazonaws.com/llava_example_kid_drawings/0.JPG\"}}]}\n",
    "       ],\n",
    "       temperature=0.01,\n",
    "       stream=True\n",
    "   )\n",
    "\n",
    "   for chat in chat_completions:\n",
    "       if chat.choices[0].delta.content is not None:\n",
    "           print(chat.choices[0].delta.content, end=\"\")\n",
    "\n",
    "query(\"http://localhost:8000/v1\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query with base64 encoded string\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path): \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def query(base_url: str, api_key: str):\n",
    "\n",
    "    client = OpenAI(\n",
    "      base_url=base_url,\n",
    "      api_key=api_key,\n",
    "    )\n",
    "\n",
    "    path = \"/mnt/local_storage/kid_drawings/0.JPG\"\n",
    "    chat_completions = client.chat.completions.create(\n",
    "        model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is the content of the image?\"}, \n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(path)}\"}}]}\n",
    "        ],\n",
    "        temperature=0.01,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chat in chat_completions:\n",
    "        if chat.choices[0].delta.content is not None:\n",
    "            print(chat.choices[0].delta.content, end=\"\")\n",
    "\n",
    "query(\"http://localhost:8000/v1\", \"NOT A REAL KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Run the Gradio application and let‚Äôs rock!\n",
    "\n",
    "To bootstrap a Gradio application, run the following command to start a Gradio application on port 7860. Notice the script uses non streaming query fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gradio_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same trick with port forwarding. And you can see the following Gradio webUI.\n",
    "\n",
    "Now let‚Äôs get ready for some verses and rhymes!\n",
    "\n",
    "Paste the following url to the input text box and click submit:\n",
    "\n",
    "https://air-example-data-2.s3.amazonaws.com/llava_example_kid_drawings/0.JPG\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/gradio_app.png\" alt=\"gradio\">\n",
    "</p>\n",
    "\n",
    "*In a land where books bloom,*  \n",
    "*A castle of stories, a dreamy room.*  \n",
    "*Where children explore,*  \n",
    "*And tales unfold, forevermore.*  \n",
    "\n",
    "*With steps that lead to the sky,*  \n",
    "*And a pool that glistens, oh so high.*  \n",
    "*Where laughter echoes,*  \n",
    "*And imagination grows,*  \n",
    "*In this magical place, where stories flow.*  \n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "Congrats! You have now served and queried [llava-hf/llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf). As a quick recap, here's what we demonstrated in this notebook:\n",
    "1. Run the model locally in a workspace\n",
    "2. Query the model with images\n",
    "3. Build a Gradio application on top.\n",
    "\n",
    "Hope that you enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
